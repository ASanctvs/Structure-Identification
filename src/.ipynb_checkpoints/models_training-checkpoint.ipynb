{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required packages to run the code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "sq1KN4nrSOu_",
    "outputId": "3e30ae80-61ec-4608-c95f-4cd2d7437c45"
   },
   "outputs": [],
   "source": [
    "def get_adjacency(sz,p,undirected):\n",
    "    '''\n",
    "    Generates a realization of an Erdős–Rényi Graph Model, undirected or directed.\n",
    "    -First generates of matrix of random floating point numbers in the range [0.0, 1.0].\n",
    "    -If those values are <=p then there is no edge between pairs\n",
    "    -Makes the matrix symmetric if the graph is undirected\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "\n",
    "        Returns:\n",
    "                adj (2darray): Adjacency matrix\n",
    "    '''\n",
    "    adj = np.random.random((sz, sz)) <= p\n",
    "    adj = np.triu(adj.astype(int))\n",
    "    np.fill_diagonal(adj,0)\n",
    "    if(undirected):\n",
    "        adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "def get_A(adj,c,rho):\n",
    "    '''\n",
    "    Generates the connectivity matrix (interaction weights) from the adjacency matrix according to the laplacian mat rule\n",
    "\n",
    "        Parameters:\n",
    "                adj (2darray): Adjacency matrix\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1\n",
    "\n",
    "        Returns:\n",
    "                A (2darray): Connectivity matrix\n",
    "    '''    \n",
    "    sz = len(adj)\n",
    "    Dvec = np.sum(adj, axis=1)\n",
    "    Dmax = np.max(Dvec)\n",
    "    ccc = c*1/Dmax\n",
    "    D = np.diag(Dvec)\n",
    "    L = D - adj\n",
    "    Ap = np.eye(sz) - ccc*L\n",
    "    A = rho * Ap\n",
    "    return A\n",
    "\n",
    "def tsg(A,tsize,x0,qsi):\n",
    "    '''\n",
    "    Generates the syntetic time series data given the connectivity matrix and the initial condiction x(0), \n",
    "    according to the dynnamical rule y(n + 1) = Ay(n) + x(n + 1)\n",
    "\n",
    "        Parameters:\n",
    "                A (2darray): Connectivity matrix\n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                x (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    sz = len(A)\n",
    "    x = np.zeros((tsize,sz))\n",
    "    \n",
    "    x[0,:] = np.ones((1,sz))*x0\n",
    "    for i in range(1,tsize):\n",
    "      for j in range(sz):\n",
    "        x[i,j] = np.dot(A[j,:],x[i-1,:]) + qsi*np.random.randn(1)\n",
    "    return x\n",
    "\n",
    "def create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected):\n",
    "    '''\n",
    "    Generates the synthectic data, extracts the features and returns the tranning/testing dataset\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1  \n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                data (2darray): Matrix containing the feature-vectors between each pair of nodes\n",
    "                target (1darray): Ground-truth - pairs are connected or disconnected\n",
    "    '''     \n",
    "    #Generate the adjacency and A matrices\n",
    "    adj = get_adjacency(sz,p,undirected)\n",
    "    A = get_A(adj,c,rho)\n",
    "   \n",
    "    #Is the graph undirected or directed\n",
    "    if(undirected):    \n",
    "        #Create data structures\n",
    "        upper = int(sz*(sz-1)/2)  #Number of elements in the upper matrix\n",
    "        data = np.zeros((200,upper))\n",
    "        target = np.zeros((1,upper))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair (of the upper matrix) and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(j+1,sz):\n",
    "                #Compute the cross correlation\n",
    "                aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                #Extracts the first negative and positive lags\n",
    "                data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                #Saves the data\n",
    "                target[0,counter] = A[j,k]\n",
    "                counter = counter + 1\n",
    "    else:\n",
    "        #Create data structures\n",
    "        dsize = (sz*sz)-sz       #Number of elements excluding the diagonal\n",
    "        data = np.zeros((200,dsize))\n",
    "        target = np.zeros((1,dsize))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(sz):\n",
    "                if(j!=k):\n",
    "                    #Computes the cross correlation\n",
    "                    aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                    #Extracts the firs negative and positive lags\n",
    "                    data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                    #Saves the data\n",
    "                    target[0,counter] = A[j,k]\n",
    "                    counter = counter + 1 \n",
    "    return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "sz = 100     #Number of nodes\n",
    "p = 0.5      #Probability of nodes being connected (Erdős–Rényi)\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "#Define the range of noise variance\n",
    "qsi = 0.1\n",
    "tsize = 300000    #Number of samples (time series size)\n",
    "x0 = 0            #Initial condition\n",
    "   \n",
    "#True if the graph is undirected, False if not\n",
    "undirected = True\n",
    "    \n",
    "#Generate a dataset\n",
    "#Data has a n x m shape, where n is the number of features and m the number of samples\n",
    "data,target = create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "data_scaled = data/np.max(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 32ms/step - loss: 0.1565 - val_loss: 0.0135\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0159 - val_loss: 0.0087\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0116 - val_loss: 0.0050\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0058 - val_loss: 0.0015\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0032 - val_loss: 0.0019\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0024 - val_loss: 9.9208e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0018 - val_loss: 9.6035e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0027 - val_loss: 0.0016\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0016 - val_loss: 5.1416e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0017 - val_loss: 3.9109e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0029 - val_loss: 8.8269e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0011 - val_loss: 7.7154e-04\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 6.9097e-04 - val_loss: 0.0016\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.1012e-04 - val_loss: 9.0587e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 1.9185e-04 - val_loss: 6.0539e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 3.9312e-04 - val_loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model\n",
    " \n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "#Split the data for trainning and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "#Reshapes the data into three dimensions\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#Train the model\n",
    "model = train_model(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 10ms/step\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "Accuracy:  100.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAf8klEQVR4nO3de3RU9d3v8fc3kwSCihfESw016EIsSogYKVEqAawV0OMj1qpVUauHoq2W42MV+jxqrSxvD/Wg4o22FLGW0iW2Whf2KJQUW6M1VIoWkIo3UlQieOEmIcn3/LEncTKZmUzCTCaZ+bzWmjV779++fH97J/Od/fvt2dvcHRERyW15mQ5AREQyT8lARESUDERERMlARERQMhARESA/0wF01MEHH+wlJSWZDkNEpEdZuXLlR+7eP155j0sGJSUl1NTUZDoMEZEexczeTVSuZiIREVEyEBERJQMRESGNfQZmNgBYABwGNAFz3f3eqHkMuBeYAOwELnP3v3d0W3v27KG2tpbPP/987wOXlOrduzfFxcUUFBRkOhQRSSCdHcgNwH+6+9/NbD9gpZk97+5rIuYZDwwKv74KPBR+75Da2lr2228/SkpKCPKLdAfuzpYtW6itrWXgwIGZDkdEEkhbM5G7v9/8Ld/dtwFrgSOiZjsbWOCBl4ADzOzwjm7r888/p1+/fkoE3YyZ0a9fP52xifQAXdJnYGYlwAnAy1FFRwAbI8ZraZswMLMpZlZjZjV1dXXxtpGaYCWldFx6luqN1dzxwh1Ub6zO6m0mo7vGlS5p/52Bme0LLAamuftn0cUxFmlzT213nwvMBSgvL9c9t6XbuXHpjTxS8wi7GnYBELIQ+Xn5NDY14jj5eV/8qzU2NdLgDa2Wb/Im3B0zI8/y2oxHrq952chp9U31MZcP5YXoFepFfWM9jd5IYV4hAA3eQMhCFBUUAbCjfgd7GvfQRFNLTPl5+eRZXsL42lt/dMzR9QVo9MaW6X3y+xDKC7WMF4QKWuKLXH90faO1F29zTPH2a5M3sbtxd8v6QhbEFG97kaKP/f699yff8vlwx4c43ub4FuQVtPn7qG+qB2i1PwtDhQw/fDh3jruTigEVCWPojLQmAzMrIEgEj7v7kzFmqQUGRIwXA5vSGVO6hEIhhg4dyp49e8jPz+fSSy9l2rRp5OXlUVNTw4IFC7jvvvsyHWZSVq1axaZNm5gwYUKHlqusrGTWrFmUl5enKbLu6eInL+bx1x5Pzcqiv+p09KtP1PyNjY3UN9a3jDc0tf5Qbk5esUTP25n1d9TOhp0Jy9usv7390068yWpJWJ34Krpre/x9jCfeZ5Fl9Y31rHh3BafOP5UVl61IeUJIWzNR+EqhXwBr3f2eOLM9DUy2wEjgU3d/P10xpVNRURGrVq3in//8J88//zxLlizh1ltvBaC8vLzHJAIIksGSJUsyHUaPUL2xOnWJQCQJDU0NVL1TlfL1prPP4BTgEmCsma0KvyaY2VQzmxqeZwnwFvAm8DPg6jTG01p1NdxxR/CeYocccghz585lzpw5uDtVVVWceeaZAPz5z3+mrKyMsrIyTjjhBLZt2wbA3XffzdChQxk2bBjTp08Hgg/lkSNHUlpayjnnnMPHH38MBN/Ab7zxRkaMGMExxxzDCy+8AMD8+fOZNGkSZ5xxBoMGDeKGG25oiem5556joqKC4cOHc95557F9+3YAXnnlFU4++WSGDRvGiBEj+PTTT7n55ptZtGgRZWVlLFq0iB07dvCd73yHk046iRNOOIGnnnoKgF27dnHBBRdQWlrK+eefz65dCb4BZal0/FOKJJJneVSWVKZ8vWlrJnL3vxC7TyByHge+l64Y4qquhnHjoL4eCgth2TKoSO0p11FHHUVTUxObN29uNX3WrFk88MADnHLKKWzfvp3evXvz7LPP8vvf/56XX36ZPn36sHXrVgAmT57M/fffz+jRo7n55pu59dZbmT17NgANDQ387W9/azkDWbp0KRAkkFdffZVevXoxePBgrrnmGoqKipg5cyZLly5ln3324a677uKee+5h+vTpnH/++SxatIiTTjqJzz77jD59+vCTn/yEmpoa5syZA8CPfvQjxo4dy7x58/jkk08YMWIEp512Go888gh9+vRh9erVrF69muHDh6d0H/YElSWVFIYKWzU95JHXqu09ejxac3txvD4Dd2/Vrm4YHtVeEd2m3eRNLW3Wycojj/xQPvmW36q5Jl58TU1NCevVkfp2tOkmsr7Qujkl0f4M5YXa1C+RXqFehCzUqo8ienshC7U6Pok07+N4fSbNYh3j5ukPTXyo5/UZdFtVVUEiaGwM3quqUp4MgJaDHemUU07huuuu46KLLmLSpEkUFxezdOlSLr/8cvr06QPAQQcdxKeffsonn3zC6NGjAbj00ks577zzWtYzadIkAE488UTeeeedlunjxo1j//33B2DIkCG8++67fPLJJ6xZs4ZTTjkFgPr6eioqKnjjjTc4/PDDOemkkwDo27dvzHo899xzPP3008yaNQsILuV97733WLFiBddeey0ApaWllJaWdnpf9VQVAyqourSKBf9YAMDkYZOpGFDB3JVzWbxmMWWHl3H/y/ezu2E3ZsZZg8/ihpODM7aqd6qoLKlM6h+7eX3nDjmXoYcMZdyCcexu2E1eXh4PTHiAKSdOabNM9cZqxi0YR31jPYWhQmafMZstO7e0fKtc8I8FfLD9Aw7b97CWuCOXTSa+yPki69S8/uZ90l59qzdWt8x/wuEn8Or7r/LB9g8AWuJLtI6OxtuvTz+27NzS5j26HvHWFb296P0QWZdn//Usm7Zt4orhV8Q9TtHHAoh77NKRCCBXk0FlZXBG0HxmUFmZ8k289dZbhEIhDjnkENauXdsyffr06UycOJElS5YwcuRIli5d2uobR7J69eoFBB3XDQ0NbaZHlrk7X//611m4cGGrdaxevTqp7bo7ixcvZvDgwW3KdOlokBCi/0GnnDiFKSdO4Y4X7qC+sZ4mmggRYsSXRrTM25F/6ub1NVs2eVm7H1gVAyoSzpdo+7HqlMx88YZTub3OLh+dHJdNXpZwfe3FEV3XeHWPlQCSiT2ZY5xKuXlvooqKoGnottvS0kRUV1fH1KlT+f73v9/mw3LDhg0MHTqUG2+8kfLyctatW8fpp5/OvHnz2LkzOHXdunUr+++/PwceeGBLf8Bjjz3WcpbQUSNHjuSvf/0rb775JgA7d+5k/fr1HHvssWzatIlXXnkFgG3bttHQ0MB+++3X0pcB8I1vfIP777+/5Uzn1VdfBeDUU0/l8ceDztPXX3+d1atXdyq+bNbcjBSyEIWhwpS19VYMqGDG12Yk9aGVzHy5oOqdqpZLYOsb67t9f09XH7vcPDOAIAGkMAns2rWLsrKylktLL7nkEq677ro2882ePZvly5cTCoUYMmQI48ePp1evXqxatYry8nIKCwuZMGECt99+O48++ihTp05l586dHHXUUfzyl7/sVGz9+/dn/vz5XHjhhezeHVw7PXPmTI455hgWLVrENddcw65duygqKmLp0qWMGTOGO++8k7KyMmbMmMFNN93EtGnTKC0txd0pKSnhmWee4aqrruLyyy+ntLSUsrIyRowYsVf7MBu19+1cuk5k/04qE3O2sFjt2t1ZeXm5Rz/cZu3atXzlK1/JUETSHh0f6S6S7VvIRma20t3j/ggod88MRCTnJNs3kYtys89ARERaUTIQERElAxERUTIQERGUDEREBCWDlPrggw+44IILOProoxkyZAgTJkxg/fr1GYtn9uzZLT9kS1bkTfVEJHcoGaSIu3POOedQWVnJhg0bWLNmDbfffjsffvhhxmLqTDIQkdyUs8kg1Y+0W758OQUFBUydOrVlWllZGaNGjeKHP/whxx9/PEOHDmXRokVA8A28srKSb37zmxx77LFcdNFFLbd7KCkp4ZZbbmH48OEMHTqUdevWAcS9lXRjYyPXX389Q4cOpbS0lPvvv5/77ruPTZs2MWbMGMaMGQPEv431H//4R4499lhGjRrFk0/GegaRiGQ9d+9RrxNPPNGjrVmzps20RF5870UvmlnkoVtDXjSzyF9878UOLR/Lvffe69OmTWsz/YknnvDTTjvNGxoa/IMPPvABAwb4pk2bfPny5d63b1/fuHGjNzY2+siRI/2FF15wd/cjjzzS77vvPnd3f+CBB/yKK65wd/cZM2b4Y4895u7uH3/8sQ8aNMi3b9/uDz74oE+aNMn37Nnj7u5btmxpWU9dXZ27u9fV1fnXvvY13759u7u733nnnX7rrbf6rl27vLi42NevX+9NTU1+3nnn+cSJE/d6f0Tq6PERkdQDajzBZ2tOnhl05Q2r/vKXv3DhhRcSCoU49NBDGT16dMuN4UaMGEFxcTF5eXmUlZW1uhV1rFtUP/fccy33DKqsrGy5lfTSpUuZOnUq+fnBD8oPOuigNnG89NJLLbexLisr49FHH+Xdd99l3bp1DBw4kEGDBmFmXHzxxWnbFyLSfeXk7SjSccOq4447jieeeKLNdE9w76dYt5uOLouc7nFuJe1J3ALb49zGetWqVboNtYjk5plB850kbxtzW8J7mnfE2LFj2b17Nz/72c9apr3yyisceOCBLFq0iMbGRurq6lixYkWn7+4Z71bSp59+Og8//HBL0mh+UlrkragT3cb67bffZsOGDQBtkoWI5IacTAaQ+nuFmxm/+93veP755zn66KM57rjj+PGPf8y3v/1tSktLGTZsGGPHjuXuu+/msMMO69Q2brrpJvbs2UNpaSnHH388N910EwBXXnklX/7yl1u28+tf/xqAKVOmMH78eMaMGdPqNtalpaWMHDmSdevW0bt3b+bOncvEiRMZNWoURx55ZEr2h4j0LLqFtaSdjo9I5rV3C+ucPTMQEZEvKBmIiEj2JIOe1tyVK3RcRHqGrEgGvXv3ZsuWLfrg6WbcnS1bttC7d+9MhyIi7ciK3xkUFxdTW1tLXV1dpkORKL1796a4uDjTYYhIO7IiGRQUFDBw4MBMhyEi0mNlRTORiIjsHSUDERFRMhARESUDERFByUBEREhjMjCzeWa22cxej1O+v5n9wcz+YWb/NLPL0xWLiIgkls4zg/nAGQnKvwescfdhQCXwUzMrTGM8IiISR9qSgbuvALYmmgXYz4Inq+wbnrchwfwiIpImmewzmAN8BdgEvAb8wN2bYs1oZlPMrMbMavQrYxGR1MtkMvgGsAr4ElAGzDGzvrFmdPe57l7u7uX9+/fvughFRHJEJpPB5cCTHngTeBs4NoPxiIjkrEwmg/eAcQBmdigwGHgrg/GIiOSstN2ozswWElwldLCZ1QK3AAUA7v4wcBsw38xeAwy40d0/Slc8IiISX9qSgbtf2E75JuD0dG1fRESSp18gi4iIkoGIiCgZiIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIoKSgYiIoGQgIiIoGYiICJCf7IxmdgRwZOQy7r4iwfzzgDOBze5+fJx5KoHZQAHwkbuPTjYeERFJnaSSgZndBZwPrAEaw5MdiJsMgPnAHGBBnHUeADwInOHu75nZIcmFLCIiqZbsmcF/AIPdfXeyK3b3FWZWkmCWbwNPuvt74fk3J7tuERFJrWT7DN4iaMpJpWOAA82sysxWmtnkeDOa2RQzqzGzmrq6uhSHISIiyZ4Z7ARWmdkyoOXswN2v3cttnwiMA4qAajN7yd3XR8/o7nOBuQDl5eW+F9sUEZEYkk0GT4dfqVRL0Gm8A9hhZiuAYUCbZCAiIumVVDJw90fNrJCgaQfgDXffs5fbfgqYY2b5QCHwVeD/7uU6RUSkE5K9mqgSeBR4BzBggJld2s6lpQuBSuBgM6sFbiHc7+DuD7v7WjP7I7AaaAJ+7u6vd7omIiLSack2E/0UON3d3wAws2OAhQRt/jG5+4XtrdTd/wf4nyRjEBGRNEn2aqKC5kQAEO7kTfXVRSIikiHJnhnUmNkvgMfC4xcBK9MTkoiIdLVkk8FVwPeAawn6DFYQ/HpYRESyQLJXE+0G7gm/REQkyyRMBmb2W3f/lpm9RnAvolbcvTRtkYmISJdp78zgB+H3M9MdiIiIZE7Cq4nc/f3w4NXu/m7kC7g6/eGJiEhXSPbS0q/HmDY+lYGIiEjmtNdncBXBGcBRZrY6omg/4K/pDExERLpOe30GvwaeBe4ApkdM3+buW9MWlYiIdKmEycDdPwU+BS4ECD+NrDewr5nt2/xgGhER6dmS6jMws7PM7F/A28CfCW5Y92wa4xIRkS6UbAfyTGAksN7dBxI8kEZ9BiIiWSLZ21HscfctZpZnZnnuvtzM7kprZGlQWdl22re+BVdfDTt3woQJbcsvuyx4ffQRfPObbcuvugrOPx82boRLLmlb/p//CWedBW+8Ad/9btvy//5vOO00WLUKpk1rW3777XDyyfDii/CjH7Utnz0byspg6VKYObNt+SOPwODB8Ic/wE9/2rb8scdgwABYtAgeeqht+RNPwMEHw/z5wSvakiXQpw88+CD89rdty6uqgvdZs+CZZ1qXFRXBs+Hzy9tug2XLWpf36weLFwfDM2ZAdXXr8uJi+NWvguFp04J9GOmYY2Du3GB4yhRYH/XYpLKyYP8BXHwx1Na2Lq+ogDvuCIbPPRe2bGldPm4c3HRTMDx+POza1br8zDPh+uuDYf3ttS3X314w3JG/veY6pUOyyeATM9uX4J5Ej5vZZqAhfWGJiEhXMvf2HylsZvsAnxPcpO4iYH/gcXffknDBNCgvL/eampqu3qyISI9mZivdvTxeebI3qtsRXllf4A8pik1ERLqJZB97+V3gJ8AugkdUGsGN645KX2giItJVku0zuB44zt0/SmcwIiKSGcleWroB2JnOQEREJHOSPTOYAbxoZi8Du5snuvu1aYlKRES6VLLJ4BHgT8BrBH0GIiKSRZJNBg3ufl1aIxERkYxJts9guZlNMbPDzeyg5ldaIxMRkS6T7JnBt8PvMyKm6dJSEZEskeyPzgamOxAREcmc9p50Ntbd/2Rmk2KVu/uT6QlLRES6UntnBqMJriI6K0aZA0oGIiJZoL0nnd0SHvyJu78dWWZmajoSEckSyV5NtDjGtCdSGYiIiGROe30GxwLHAftH9Rv0JXgWsoiIZIH2zgwGA2cCBxD0GzS/hgP/O9GCZjbPzDab2evtzHeSmTWaWYxnOYmISFdor8/gKeApM6tw9+pE88YwH5gDLIg3g5mFgLuA/9fBdYuISAol22dwjpn1NbMCM1tmZh+Z2cWJFnD3FcDWdtZ7DUF/xOYk4xARkTRINhmc7u6fETQZ1QLHAD/cmw2b2RHAOcDDScw7xcxqzKymrq5ubzYrIiIxJJsMCsLvE4CF7t7eN/5kzAZudPfG9mZ097nuXu7u5f3790/BpkVEJFKy9yb6g5mtI3js5dVm1h/4fC+3XQ78xswADgYmmFmDu/9+L9crIiIdlOy9iaab2V3AZ+7eaGY7gbP3ZsOR9zsys/nAM0oEIiKZkbCZyMxuiBg9rblJx913AAmfcmZmC4FqYLCZ1ZrZFWY21cym7m3QIiKSWubu8QvN/u7uw6OHY413lfLycq+pqenqzYqI9GhmttLdy+OVt9eBbHGGY42LiEgP1V4y8DjDscZFRKSHaq8DeZiZfUZwFlAUHiY8rnsTiYhkifZuRxHqqkBERCRzkv3RmYiIZDElAxERUTIQERElAxERQclARERQMhAREZQMREQEJQMREUHJQEREUDIQERGUDEREBCUDERFByUBERFAyEBERlAxERAQlAxERQclARERQMhAREZQMREQEJQMREUHJQEREUDIQERGUDEREBCUDERFByUBERFAyEBERlAxERIQ0JgMzm2dmm83s9TjlF5nZ6vDrRTMblq5YREQksXSeGcwHzkhQ/jYw2t1LgduAuWmMRUREEshP14rdfYWZlSQofzFi9CWgOF2xiIhIYt2lz+AK4Nl4hWY2xcxqzKymrq6uC8MSEckNGU8GZjaGIBncGG8ed5/r7uXuXt6/f/+uC05EJEekrZkoGWZWCvwcGO/uWzIZi4hILsvYmYGZfRl4ErjE3ddnKg4REUnjmYGZLQQqgYPNrBa4BSgAcPeHgZuBfsCDZgbQ4O7l6YpHRETiS+fVRBe2U34lcGW6ti8iIsnLeAeyiIhknpKBiIgoGYiIiJKBiIigZCAiIigZiIgISgYiIoKSgYiIoGQgIiIoGYiICEoGIiKCkoGIiKBkICIiKBmIiAhKBiIigpKBiIigZCAiIigZiIgISgYiIoKSgYiIoGTQfVVXwx13BO8iImmWn+kAJIbqahg3DurrobAQli2DiopMRyUiWUxnBt1RVVWQCBobg/eqqkxHJCJZTsmgO6qsDM4IQqHgvbIy0xGJSJZTM1F3VFERNA1VVQWJQE1EIpJmSgbdVUWFkoCIdBk1E3UnuoJIRDJEZwbdha4gEpEM0plBd6EriEQkg5QMuovOXEGkZiURSRE1E3UXHb2CqCPNStXVujJJRBJKWzIws3nAmcBmdz8+RrkB9wITgJ3AZe7+93TFQ3U13H03vPQSfPYZNDR8UdbUFLwXFUFBAezYETTXNH9Tb266ycuDXr2+aM4pLAyWq68HdzAL5gmFguH6+qC8oKD1eF5esM3IZZpjKCyEmTODsvz8YDsNDW3nb54GsGsXjBoVlEWuzz0ob2z8oq6hUPCKtf1kxkMh2Hdf2LMnePXrF6z/44+DmCL3QWT80SLX17xPGxq+GI+3XLz1xopv167E+zt6+4mOaeTxifybaD6+hx4KM2bAlCmJ/w5Fuql0nhnMB+YAC+KUjwcGhV9fBR4Kv6dedTWcemrsD5dI27a1Ho81f/MHQLzyWJKdr6PzRmr+sGpPY2Pr5NBRjY2wdesX4//+d+fX1by+yH0aPd6Z9UXG19Htt7f/Y5U3NMA778B3vxuMKyFID5S2PgN3XwEk+q88G1jggZeAA8zs8LQEU1XV+Q9ZkY5YvDjTEYh0SiY7kI8ANkaM14antWFmU8ysxsxq6urqOr6lysrgVF8k3c49N9MRiHRKJj8hLcY0jzWju89193J3L+/fv3/Ht1RRAQ89FDsh5OcHbcB7o7kdPj8/eEWK3mbz1ULN283P/2I8Px/69AnasKNjTDR/nz6xy5vHi4q+mCdWebLj7SXUvLzY+zN6ueY4oqdbrD8J2j8+8dbXvGxn6hN5TJv7CeLNV1QEJSXwyCNqIpIeK5NXE9UCAyLGi4FNadvalCkwdGjQZNSvH2zZ0vrqmupqWBDu3pg8OXiPnrd5WvRw9BU60VfvdOZqnu56BVDkfurbN4jxS1+CG25oHefcuUGTybnnBvs+Xn3i7avoYxQ9PdYxjI5v8uTkrspK5phGzhtv2yI9mLnH/DKempWblQDPxLmaaCLwfYKrib4K3OfuI9pbZ3l5udfU1KQ6VBGRrGZmK929PF55Oi8tXQhUAgebWS1wC1AA4O4PA0sIEsGbBJeWXp6uWEREJLG0JQN3v7Cdcge+l67ti4hI8nSJjYiIKBmIiIiSgYiIoGQgIiKk+dLSdDCzOuDdTi5+MPBRCsPpKVTv3JGLdYbcrHdH63yku8f91W6PSwZ7w8xqEl1nm61U79yRi3WG3Kx3quusZiIREVEyEBGR3EsGczMdQIao3rkjF+sMuVnvlNY5p/oMREQktlw7MxARkRiUDEREJHeSgZmdYWZvmNmbZjY90/GkipkNMLPlZrbWzP5pZj8ITz/IzJ43s3+F3w+MWGZGeD+8YWbfyFz0e8fMQmb2qpk9Ex7PhTofYGZPmNm68DGvyJF6/5/w3/frZrbQzHpnY73NbJ6ZbTaz1yOmdbieZnaimb0WLrvPLN6ToyK4e9a/gBCwATgKKAT+AQzJdFwpqtvhwPDw8H7AemAIcDcwPTx9OnBXeHhIuP69gIHh/RLKdD06WffrgF8TPDODHKnzo8CV4eFC4IBsrzfB43DfBorC478FLsvGegOnAsOB1yOmdbiewN+ACoInSj4LjG9v27lyZjACeNPd33L3euA3wNkZjikl3P19d/97eHgbsJbgn+dsgg8Owu//ER4+G/iNu+9297cJnifR7kOFuhszKwYmAj+PmJztde5L8GHxCwB3r3f3T8jyeoflA0Vmlg/0IXgqYtbV291XAFujJneonmZ2ONDX3as9yAwLIpaJK1eSwRHAxojx2vC0rBJ+stwJwMvAoe7+PgQJAzgkPFu27IvZwA1AU8S0bK/zUUAd8Mtw89jPzWwfsrze7v5vYBbwHvA+8Km7P0eW1ztCR+t5RHg4enpCuZIMYrWXZdU1tWa2L7AYmObunyWaNca0HrUvzOxMYLO7r0x2kRjTelSdw/IJmhAecvcTgB0EzQbxZEW9w23kZxM0hXwJ2MfMLk60SIxpPa7eSYhXz07VP1eSQS0wIGK8mOA0MyuYWQFBInjc3Z8MT/4wfLpI+H1zeHo27ItTgP9lZu8QNPmNNbNfkd11hqAete7+cnj8CYLkkO31Pg14293r3H0P8CRwMtlf72YdrWdteDh6ekK5kgxeAQaZ2UAzKwQuAJ7OcEwpEb5K4BfAWne/J6LoaeDS8PClwFMR0y8ws15mNhAYRNDZ1GO4+wx3L3b3EoJj+Sd3v5gsrjOAu38AbDSzweFJ44A1ZHm9CZqHRppZn/Df+ziCvrFsr3ezDtUz3JS0zcxGhvfX5Ihl4st073kX9tJPILjSZgPwX5mOJ4X1GkVwCrgaWBV+TQD6AcuAf4XfD4pY5r/C++ENkrjKoDu/gEq+uJoo6+sMlAE14eP9e+DAHKn3rcA64HXgMYIraLKu3sBCgn6RPQTf8K/oTD2B8vC+2gDMIXy3iUQv3Y5CRERypplIREQSUDIQERElAxERUTIQERGUDEREBCUDERFByUBERID/D9koJHigGYgKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the testing results\n",
    "#Disconnected samples\n",
    "idd = y_test < 2\n",
    "idd = np.squeeze(idd)\n",
    "\n",
    "#Connected samples\n",
    "idc = y_test > 1\n",
    "idc = np.squeeze(idc)\n",
    "\n",
    "#Threshold in the middle\n",
    "threshold = np.ones((X_test.shape[0]))*1.5\n",
    "\n",
    "#Predict the samples seperaly\n",
    "dpred = model.predict(X_test[idd,:,:])\n",
    "cpred =model.predict(X_test[idc,:,:])\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(np.arange(0,len(dpred)),dpred,'.r')\n",
    "plt.plot(np.arange(len(dpred),len(dpred)+len(cpred)),cpred,'.g')\n",
    "plt.plot(threshold,'--b')\n",
    "plt.legend([\"Disconnected\",\"Connected\"],loc='best')\n",
    "plt.ylabel(\"Estimation\")\n",
    "\n",
    "trued = np.sum(dpred<1.5)\n",
    "truec = np.sum(cpred>1.5)\n",
    "true = trued + truec\n",
    "accuracy_nn = true/(len(dpred)+len(cpred))*100\n",
    "print(\"Accuracy:  \" + str(accuracy_nn) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN model\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='../models/new_cnn_model'\n",
    "if(save):\n",
    "    save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Train and test a SVM with linear kernel\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(data_scaled[:,:].T, y.flatten())\n",
    "pred = clf.predict(data_scaled.T)\n",
    "true = y.flatten()\n",
    "print(\"Accuracy:  \" + str(sum(pred==true)/len(pred)*100) + \" %\")\n",
    "\n",
    "#Save the model?\n",
    "save=True\n",
    "if(save):\n",
    "    model_name='../models/new_svm_model.sav'\n",
    "    pickle.dump(clf, open(model_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models select the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 33ms/step - loss: 0.1265 - val_loss: 0.0120\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0153 - val_loss: 0.0097\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0110 - val_loss: 0.0037\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0055 - val_loss: 0.0016\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0044 - val_loss: 0.0023\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0030 - val_loss: 0.0013\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0024 - val_loss: 0.0019\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0023 - val_loss: 9.6068e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0023 - val_loss: 0.0010\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0022 - val_loss: 0.0026\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0023 - val_loss: 2.6913e-04\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0020 - val_loss: 4.6451e-04\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0016 - val_loss: 8.4779e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0019 - val_loss: 0.0010\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0016 - val_loss: 5.9153e-04\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0014 - val_loss: 1.3503e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0093 - val_loss: 0.0031\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 9.1957e-04 - val_loss: 5.7421e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 3.0769e-04 - val_loss: 5.3191e-04\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.7394e-04 - val_loss: 7.0070e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 1.5209e-04 - val_loss: 7.5631e-04\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 1.0951e-04 - val_loss: 3.6018e-04\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0010 - val_loss: 5.3652e-04\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 32ms/step - loss: 0.1442 - val_loss: 0.0120\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0108 - val_loss: 0.0053\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0078 - val_loss: 0.0056\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0016 - val_loss: 9.6136e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 8.5424e-04 - val_loss: 6.3107e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 5.3776e-04 - val_loss: 0.0011\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 7.4777e-04 - val_loss: 8.5834e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 2.6808e-04 - val_loss: 0.0011\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 2.3240e-04 - val_loss: 7.7311e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 2.0428e-04 - val_loss: 4.8765e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 3.7678e-04 - val_loss: 9.6175e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 5.4464e-04 - val_loss: 2.8916e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.7496e-04 - val_loss: 1.3277e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.7696e-04 - val_loss: 8.2115e-04\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 9.1765e-05 - val_loss: 5.3126e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 5.7827e-05 - val_loss: 8.0280e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 4.5819e-05 - val_loss: 7.6907e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 6.0746e-05 - val_loss: 7.0294e-04\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 5.4512e-05 - val_loss: 6.5789e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 5.3205e-05 - val_loss: 6.1151e-04\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 38ms/step - loss: 0.1253 - val_loss: 0.0111\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0161 - val_loss: 0.0110\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0108 - val_loss: 0.0076\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0070 - val_loss: 0.0023\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0042 - val_loss: 9.6155e-04\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0029 - val_loss: 9.5436e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0018 - val_loss: 5.5037e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0021 - val_loss: 0.0032\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0021 - val_loss: 3.6710e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0013 - val_loss: 8.5097e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0015 - val_loss: 9.7847e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0017 - val_loss: 8.5557e-04\n",
      "16/16 [==============================] - 0s 11ms/step\n",
      "16/16 [==============================] - 0s 11ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.1331 - val_loss: 0.0114\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0150 - val_loss: 0.0082\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0103 - val_loss: 0.0086\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0078 - val_loss: 0.0025\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0043 - val_loss: 0.0012\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0029 - val_loss: 9.9469e-04\n",
      "Epoch 8/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0020 - val_loss: 9.9674e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0024 - val_loss: 0.0013\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0027 - val_loss: 5.1610e-04\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0020 - val_loss: 7.1010e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0019 - val_loss: 5.0683e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0015 - val_loss: 3.4914e-04\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0011 - val_loss: 5.1513e-04\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 7.2344e-04 - val_loss: 8.1739e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 3.9891e-04 - val_loss: 3.0710e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 2.9460e-04 - val_loss: 7.7331e-05\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 7.8262e-05 - val_loss: 1.2343e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.7675e-05 - val_loss: 6.7437e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 5.2096e-05 - val_loss: 8.1788e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.2301e-05 - val_loss: 8.5683e-04\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 4.6579e-05 - val_loss: 8.2822e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 7.9810e-05 - val_loss: 9.3778e-04\n",
      "16/16 [==============================] - 0s 11ms/step\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 33ms/step - loss: 0.1609 - val_loss: 0.0157\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0111 - val_loss: 0.0050\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 4s 39ms/step - loss: 0.0072 - val_loss: 0.0026\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0025 - val_loss: 0.0021\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0016 - val_loss: 0.0011\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 8.2381e-04 - val_loss: 0.0016\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 7.0527e-04 - val_loss: 0.0011\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.7778e-04 - val_loss: 0.0012\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 2.5786e-04 - val_loss: 8.4735e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.4604e-04 - val_loss: 0.0012\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 4.9592e-04 - val_loss: 0.0051\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 6.7317e-04 - val_loss: 0.0029\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 3.6794e-04 - val_loss: 7.3546e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 4.4751e-04 - val_loss: 1.0623e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 2.5673e-04 - val_loss: 1.8762e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.4035e-04 - val_loss: 0.0013\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.5327e-04 - val_loss: 2.3499e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 2.0103e-04 - val_loss: 9.1559e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 0.0014 - val_loss: 8.8957e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.6194e-04 - val_loss: 4.5574e-04\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.8632e-04 - val_loss: 5.4930e-04\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "16/16 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "nruns=5\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "\n",
    "for i in range(nruns):\n",
    "    #Split the data for trainning and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Reshapes the data into three dimensions\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    #Train the model\n",
    "    model = train_model(X_train,X_test,y_train,y_test)\n",
    "    \n",
    "    #Save the data\n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Compute the performance according to a chosen metric on the testing set\n",
    "    #I chose the identifiability in this case\n",
    "    #Disconnected samples\n",
    "    idd = y_test < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:])\n",
    "    cpred =model.predict(X_test[idc,:,:])\n",
    "    \n",
    "    id_gap = min(cpred)-max(dpred)\n",
    "    \n",
    "    #Add to performance list\n",
    "    performance_list.append(id_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance model index:  3\n"
     ]
    }
   ],
   "source": [
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "print(\"Best performance model index:  \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='../models/new_cnn_model'\n",
    "if(save):\n",
    "    save_model(best_model,model_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
