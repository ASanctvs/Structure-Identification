{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required packages to run the code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "sq1KN4nrSOu_",
    "outputId": "3e30ae80-61ec-4608-c95f-4cd2d7437c45"
   },
   "outputs": [],
   "source": [
    "def get_adjacency(sz,p,undirected):\n",
    "    '''\n",
    "    Generates a realization of an Erdős–Rényi Graph Model, undirected or directed.\n",
    "    -First generates of matrix of random floating point numbers in the range [0.0, 1.0].\n",
    "    -If those values are <=p then there is no edge between pairs\n",
    "    -Makes the matrix symmetric if the graph is undirected\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "\n",
    "        Returns:\n",
    "                adj (2darray): Adjacency matrix\n",
    "    '''\n",
    "    adj = np.random.random((sz, sz)) <= p\n",
    "    adj = np.triu(adj.astype(int))\n",
    "    np.fill_diagonal(adj,0)\n",
    "    if(undirected):\n",
    "        adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "def get_A(adj,c,rho):\n",
    "    '''\n",
    "    Generates the connectivity matrix (interaction weights) from the adjacency matrix according to the laplacian mat rule\n",
    "\n",
    "        Parameters:\n",
    "                adj (2darray): Adjacency matrix\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1\n",
    "\n",
    "        Returns:\n",
    "                A (2darray): Connectivity matrix\n",
    "    '''    \n",
    "    sz = len(adj)\n",
    "    Dvec = np.sum(adj, axis=1)\n",
    "    Dmax = np.max(Dvec)\n",
    "    ccc = c*1/Dmax\n",
    "    D = np.diag(Dvec)\n",
    "    L = D - adj\n",
    "    Ap = np.eye(sz) - ccc*L\n",
    "    A = rho * Ap\n",
    "    return A\n",
    "\n",
    "def tsg(A,tsize,x0,qsi):\n",
    "    '''\n",
    "    Generates the syntetic time series data given the connectivity matrix and the initial condiction x(0), \n",
    "    according to the dynnamical rule y(n + 1) = Ay(n) + x(n + 1)\n",
    "\n",
    "        Parameters:\n",
    "                A (2darray): Connectivity matrix\n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                x (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    sz = len(A)\n",
    "    x = np.zeros((tsize,sz))\n",
    "    \n",
    "    x[0,:] = np.ones((1,sz))*x0\n",
    "    for i in range(1,tsize):\n",
    "      for j in range(sz):\n",
    "        x[i,j] = np.dot(A[j,:],x[i-1,:]) + qsi*np.random.randn(1)\n",
    "    return x\n",
    "\n",
    "def create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected):\n",
    "    '''\n",
    "    Generates the synthectic data, extracts the features and returns the tranning/testing dataset\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1  \n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                data (2darray): Matrix containing the feature-vectors between each pair of nodes\n",
    "                target (1darray): Ground-truth - pairs are connected or disconnected\n",
    "    '''     \n",
    "    #Generate the adjacency and A matrices\n",
    "    adj = get_adjacency(sz,p,undirected)\n",
    "    A = get_A(adj,c,rho)\n",
    "   \n",
    "    #Is the graph undirected or directed\n",
    "    if(undirected):    \n",
    "        #Create data structures\n",
    "        upper = int(sz*(sz-1)/2)  #Number of elements in the upper matrix\n",
    "        data = np.zeros((200,upper))\n",
    "        target = np.zeros((1,upper))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair (of the upper matrix) and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(j+1,sz):\n",
    "                #Compute the cross correlation\n",
    "                aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                #Extracts the first negative and positive lags\n",
    "                data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                #Saves the data\n",
    "                target[0,counter] = A[j,k]\n",
    "                counter = counter + 1\n",
    "    else:\n",
    "        #Create data structures\n",
    "        dsize = (sz*sz)-sz       #Number of elements excluding the diagonal\n",
    "        data = np.zeros((200,dsize))\n",
    "        target = np.zeros((1,dsize))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(sz):\n",
    "                if(j!=k):\n",
    "                    #Computes the cross correlation\n",
    "                    aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                    #Extracts the firs negative and positive lags\n",
    "                    data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                    #Saves the data\n",
    "                    target[0,counter] = A[j,k]\n",
    "                    counter = counter + 1 \n",
    "    return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "sz = 100     #Number of nodes\n",
    "p = 0.5      #Probability of nodes being connected (Erdős–Rényi)\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "#Define the range of noise variance\n",
    "qsi = 0.1\n",
    "tsize = 300000    #Number of samples (time series size)\n",
    "x0 = 0            #Initial condition\n",
    "   \n",
    "#True if the graph is undirected, False if not\n",
    "undirected = True\n",
    "    \n",
    "#Generate a dataset\n",
    "#Data has a n x m shape, where n is the number of features and m the number of samples\n",
    "data,target = create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "data_scaled = data/np.max(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 31ms/step - loss: 0.1490 - val_loss: 0.0124\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0169 - val_loss: 0.0086\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0113 - val_loss: 0.0051\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0069 - val_loss: 0.0021\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0043 - val_loss: 0.0010\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0029 - val_loss: 5.3826e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0035 - val_loss: 6.0166e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0026 - val_loss: 6.5214e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0021 - val_loss: 5.4498e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0017 - val_loss: 7.7884e-04\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0020 - val_loss: 8.4863e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0019 - val_loss: 9.3429e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0015 - val_loss: 2.3646e-04\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0016 - val_loss: 4.8672e-05\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0016 - val_loss: 2.5869e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0016 - val_loss: 4.5488e-05\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0010 - val_loss: 3.7168e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 5.4788e-04 - val_loss: 1.3791e-04\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 4.1381e-04 - val_loss: 1.4384e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 2.3579e-04 - val_loss: 5.7075e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.1615e-04 - val_loss: 3.5874e-05\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 7.0636e-05 - val_loss: 8.7890e-05\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.7126e-04 - val_loss: 1.1123e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 3.9651e-04 - val_loss: 9.3072e-05\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.3058e-04 - val_loss: 9.2457e-04\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.2621e-04 - val_loss: 1.4111e-04\n",
      "Epoch 27/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 9.0992e-05 - val_loss: 6.5085e-05\n",
      "Epoch 28/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 5.6578e-05 - val_loss: 9.8443e-05\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model\n",
    " \n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "#Split the data for trainning and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "#Reshapes the data into three dimensions\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#Train the model\n",
    "model = train_model(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 10ms/step\n",
      "15/15 [==============================] - 0s 10ms/step\n",
      "Accuracy:  100.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the testing results\n",
    "#Disconnected samples\n",
    "idd = y_test < 2\n",
    "idd = np.squeeze(idd)\n",
    "\n",
    "#Connected samples\n",
    "idc = y_test > 1\n",
    "idc = np.squeeze(idc)\n",
    "\n",
    "#Threshold in the middle\n",
    "threshold = np.ones((X_test.shape[0]))*1.5\n",
    "\n",
    "#Predict the samples seperaly\n",
    "dpred = model.predict(X_test[idd,:,:])\n",
    "cpred =model.predict(X_test[idc,:,:])\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(np.arange(0,len(dpred)),dpred,'.r')\n",
    "plt.plot(np.arange(len(dpred),len(dpred)+len(cpred)),cpred,'.g')\n",
    "plt.plot(threshold,'--b')\n",
    "plt.legend([\"Disconnected\",\"Connected\"],loc='best')\n",
    "plt.ylabel(\"Estimation\")\n",
    "\n",
    "trued = np.sum(dpred<1.5)\n",
    "truec = np.sum(cpred>1.5)\n",
    "true = trued + truec\n",
    "accuracy_nn = true/(len(dpred)+len(cpred))*100\n",
    "print(\"Accuracy:  \" + str(accuracy_nn) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN model\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='model'\n",
    "if(save):\n",
    "    save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Train and test a SVM with linear kernel\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(data_scaled[:,:].T, y.flatten())\n",
    "pred = clf.predict(data_scaled.T)\n",
    "true = y.flatten()\n",
    "print(\"Accuracy:  \" + str(sum(pred==true)/len(pred)*100) + \" %\")\n",
    "\n",
    "#Save the model?\n",
    "save=False\n",
    "if(save):\n",
    "    filename = 'svm_model.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models select the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 31ms/step - loss: 0.1578 - val_loss: 0.0117\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0155 - val_loss: 0.0092\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0112 - val_loss: 0.0070\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0093 - val_loss: 0.0048\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0038 - val_loss: 4.5700e-04\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0031 - val_loss: 0.0014\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0024 - val_loss: 1.3265e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0024 - val_loss: 3.6456e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0022 - val_loss: 8.5671e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0027 - val_loss: 1.1698e-04\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0017 - val_loss: 2.2344e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0017 - val_loss: 1.2674e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0020 - val_loss: 1.7380e-04\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 0.0018 - val_loss: 2.0375e-04\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0016 - val_loss: 5.3165e-05\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0015 - val_loss: 1.5433e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 4s 38ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 5.6033e-04 - val_loss: 6.0831e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 4.6762e-04 - val_loss: 1.0085e-04\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 5.9600e-05 - val_loss: 1.8230e-06\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 4s 38ms/step - loss: 5.7305e-05 - val_loss: 5.7077e-05\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 3.7608e-05 - val_loss: 3.9072e-05\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 4.0466e-05 - val_loss: 4.7078e-06\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 9.1458e-05 - val_loss: 3.5622e-06\n",
      "Epoch 27/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 6.5281e-05 - val_loss: 3.2533e-06\n",
      "Epoch 28/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 5.5981e-05 - val_loss: 1.8489e-05\n",
      "Epoch 29/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 4.4344e-05 - val_loss: 1.9000e-05\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "15/15 [==============================] - 0s 11ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 38ms/step - loss: 0.1547 - val_loss: 0.0165\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0180 - val_loss: 0.0107\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0125 - val_loss: 0.0067\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0092 - val_loss: 0.0031\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 0.0056 - val_loss: 0.0013\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0035 - val_loss: 9.0697e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 0.0030 - val_loss: 5.9596e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0026 - val_loss: 4.5746e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0023 - val_loss: 2.0766e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.0023 - val_loss: 1.2882e-04\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 0.0021 - val_loss: 2.1958e-04\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0017 - val_loss: 1.1258e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 0.0016 - val_loss: 0.0014\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0015 - val_loss: 1.2304e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 9.9434e-04 - val_loss: 5.7600e-04\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 5.5858e-04 - val_loss: 3.9736e-04\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 1.5197e-04 - val_loss: 1.5149e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 8.6490e-05 - val_loss: 3.4200e-05\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.2648e-04 - val_loss: 0.0082\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0045 - val_loss: 1.4516e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 5.9236e-05 - val_loss: 5.3646e-05\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 2.3842e-05 - val_loss: 9.8470e-06\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 3.9898e-05 - val_loss: 2.9419e-06\n",
      "Epoch 27/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 6.0831e-05 - val_loss: 1.4631e-05\n",
      "Epoch 28/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 1.3693e-05 - val_loss: 6.7011e-06\n",
      "Epoch 29/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.7863e-05 - val_loss: 1.2621e-05\n",
      "Epoch 30/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 3.2134e-05 - val_loss: 1.2781e-05\n",
      "Epoch 31/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 1.0854e-05 - val_loss: 4.5875e-05\n",
      "Epoch 32/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.7682e-05 - val_loss: 4.3195e-05\n",
      "Epoch 33/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 5.8976e-05 - val_loss: 1.0420e-05\n",
      "17/17 [==============================] - 0s 11ms/step\n",
      "15/15 [==============================] - 0s 11ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 0.1357 - val_loss: 0.0117\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0154 - val_loss: 0.0078\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0106 - val_loss: 0.0051\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0076 - val_loss: 0.0021\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0053 - val_loss: 0.0018\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0034 - val_loss: 6.5671e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0018 - val_loss: 1.2652e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0018 - val_loss: 1.4469e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 0.0015 - val_loss: 0.0010\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0013 - val_loss: 3.4217e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 0.0011 - val_loss: 9.0511e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 5.2793e-04 - val_loss: 1.7676e-04\n",
      "Epoch 14/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 4s 36ms/step - loss: 2.4844e-04 - val_loss: 6.8180e-05\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 2.7618e-04 - val_loss: 1.1285e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.4572e-04 - val_loss: 3.5988e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.2365e-04 - val_loss: 6.6781e-05\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 1.3350e-04 - val_loss: 4.9298e-05\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 9.6659e-05 - val_loss: 7.1072e-05\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 8.3123e-05 - val_loss: 2.7007e-04\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 6.8647e-05 - val_loss: 3.6063e-05\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 5.8258e-05 - val_loss: 7.8272e-05\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 8.6781e-05 - val_loss: 1.2193e-04\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 1.1568e-04 - val_loss: 1.4929e-04\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 4.5922e-05 - val_loss: 5.7275e-05\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 2.0194e-04 - val_loss: 0.0021\n",
      "Epoch 27/1000\n",
      "99/99 [==============================] - 4s 35ms/step - loss: 3.6288e-04 - val_loss: 1.8713e-05\n",
      "Epoch 28/1000\n",
      "99/99 [==============================] - 4s 37ms/step - loss: 5.6447e-05 - val_loss: 3.1764e-05\n",
      "Epoch 29/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 5.6479e-05 - val_loss: 2.0376e-05\n",
      "Epoch 30/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 7.1520e-05 - val_loss: 2.7952e-05\n",
      "Epoch 31/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 5.4813e-05 - val_loss: 3.1305e-05\n",
      "Epoch 32/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.9773e-04 - val_loss: 1.3011e-04\n",
      "Epoch 33/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.1298e-04 - val_loss: 9.4657e-05\n",
      "Epoch 34/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 5.7649e-05 - val_loss: 2.6666e-05\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "15/15 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 32ms/step - loss: 0.1380 - val_loss: 0.0136\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0170 - val_loss: 0.0089\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0066 - val_loss: 0.0061\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0046 - val_loss: 0.0011\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 29ms/step - loss: 0.0030 - val_loss: 3.3653e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0025 - val_loss: 3.5291e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0021 - val_loss: 2.3109e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 0.0014 - val_loss: 3.2176e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0010 - val_loss: 6.7862e-05\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 9.2197e-04 - val_loss: 1.9955e-04\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 6.8549e-04 - val_loss: 5.8960e-05\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 3.8778e-04 - val_loss: 1.3228e-05\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 35ms/step - loss: 2.2269e-04 - val_loss: 4.8895e-05\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 1.8307e-04 - val_loss: 1.0492e-04\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.2842e-04 - val_loss: 2.7817e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.3972e-04 - val_loss: 3.5503e-04\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 9.9949e-05 - val_loss: 1.0803e-05\n",
      "Epoch 19/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 1.2347e-04 - val_loss: 2.4718e-05\n",
      "Epoch 20/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 7.4713e-05 - val_loss: 1.6475e-05\n",
      "Epoch 21/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 5.4151e-05 - val_loss: 2.1666e-05\n",
      "Epoch 22/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 8.0192e-05 - val_loss: 1.8923e-05\n",
      "Epoch 23/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 6.9454e-05 - val_loss: 8.0429e-05\n",
      "Epoch 24/1000\n",
      "99/99 [==============================] - 3s 34ms/step - loss: 4.3442e-05 - val_loss: 1.3233e-04\n",
      "Epoch 25/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 8.1451e-05 - val_loss: 6.9211e-06\n",
      "Epoch 26/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 6.2542e-05 - val_loss: 4.6600e-05\n",
      "Epoch 27/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 6.8776e-05 - val_loss: 7.1711e-05\n",
      "Epoch 28/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 3.5388e-05 - val_loss: 3.9908e-06\n",
      "Epoch 29/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 4.2790e-05 - val_loss: 1.6241e-05\n",
      "Epoch 30/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 6.9132e-05 - val_loss: 2.5624e-04\n",
      "Epoch 31/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 3.2292e-05 - val_loss: 1.3692e-05\n",
      "Epoch 32/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 2.9794e-04 - val_loss: 0.0013\n",
      "Epoch 33/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 2.5233e-04 - val_loss: 1.1752e-04\n",
      "Epoch 34/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 1.1786e-04 - val_loss: 8.1689e-06\n",
      "Epoch 35/1000\n",
      "99/99 [==============================] - 3s 33ms/step - loss: 7.4004e-05 - val_loss: 2.0980e-04\n",
      "17/17 [==============================] - 0s 10ms/step\n",
      "15/15 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "99/99 [==============================] - 4s 32ms/step - loss: 0.1464 - val_loss: 0.0146\n",
      "Epoch 2/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 0.0184 - val_loss: 0.0084\n",
      "Epoch 3/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0116 - val_loss: 0.0053\n",
      "Epoch 4/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0067 - val_loss: 0.0022\n",
      "Epoch 5/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0042 - val_loss: 0.0011\n",
      "Epoch 6/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 0.0026 - val_loss: 4.9257e-04\n",
      "Epoch 7/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 9.1281e-04 - val_loss: 6.7821e-04\n",
      "Epoch 8/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 7.1394e-04 - val_loss: 6.7103e-04\n",
      "Epoch 9/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 4.5869e-04 - val_loss: 4.9618e-04\n",
      "Epoch 10/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 3.4586e-04 - val_loss: 5.1944e-05\n",
      "Epoch 11/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 1.9277e-04 - val_loss: 4.8427e-05\n",
      "Epoch 12/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 1.3735e-04 - val_loss: 2.9551e-04\n",
      "Epoch 13/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 1.4252e-04 - val_loss: 7.3036e-05\n",
      "Epoch 14/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 2.2838e-04 - val_loss: 1.1733e-04\n",
      "Epoch 15/1000\n",
      "99/99 [==============================] - 3s 32ms/step - loss: 7.4519e-05 - val_loss: 8.5929e-05\n",
      "Epoch 16/1000\n",
      "99/99 [==============================] - 3s 30ms/step - loss: 6.3816e-05 - val_loss: 1.0488e-04\n",
      "Epoch 17/1000\n",
      "99/99 [==============================] - 3s 31ms/step - loss: 7.0605e-05 - val_loss: 9.2334e-05\n",
      "Epoch 18/1000\n",
      "99/99 [==============================] - 4s 36ms/step - loss: 3.3911e-04 - val_loss: 7.6052e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 11ms/step\n",
      "15/15 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "nruns=5\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "\n",
    "for i in range(nruns):\n",
    "    #Split the data for trainning and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Reshapes the data into three dimensions\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    #Train the model\n",
    "    model = train_model(X_train,X_test,y_train,y_test)\n",
    "    \n",
    "    #Save the data\n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Compute the performance according to a chosen metric on the testing set\n",
    "    #I chose the identifiability in this case\n",
    "    #Disconnected samples\n",
    "    idd = y_test < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:])\n",
    "    cpred =model.predict(X_test[idc,:,:])\n",
    "    \n",
    "    id_gap = min(cpred)-max(dpred)\n",
    "    \n",
    "    #Add to performance list\n",
    "    performance_list.append(id_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performance model index:  1\n"
     ]
    }
   ],
   "source": [
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "print(\"Best performance model index:  \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='model'\n",
    "if(save):\n",
    "    save_model(best_model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
