{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import callbacks\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "from scipy import signal\n",
    "import pickle\n",
    "import sklearn\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "sq1KN4nrSOu_",
    "outputId": "80bfd22e-9789-48fd-eede-7774185921d0"
   },
   "outputs": [],
   "source": [
    "def get_adjacency(sz,p,undirected):\n",
    "    '''\n",
    "    Generates a realization of an Erdős–Rényi Graph Model, undirected or directed.\n",
    "    -First generates of matrix of random floating point numbers in the range [0.0, 1.0].\n",
    "    -If those values are <=p then there is no edge between pairs\n",
    "    -If the graph is undirected it mirrors the adjacency matrix\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "\n",
    "        Returns:\n",
    "                adj (2darray): Adjacency matrix\n",
    "    '''\n",
    "    adj = np.random.random((sz, sz)) <= p\n",
    "    adj = np.triu(adj.astype(int))\n",
    "    np.fill_diagonal(adj,0)\n",
    "    if(undirected):\n",
    "        adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "def get_A(adj,c,rho):\n",
    "    '''\n",
    "    Generates the connectivity matrix (interaction weights) from the adjacency matrix according to the laplacian mat rule.\n",
    "\n",
    "        Parameters:\n",
    "                adj (2darray): Adjacency matrix\n",
    "                c,rho (int): Numbers between 0 and 1, so the spectral radius is < 1\n",
    "\n",
    "        Returns:\n",
    "                A (2darray): Connectivity matrix\n",
    "    '''    \n",
    "    sz = len(adj)\n",
    "    Dvec = np.sum(adj, axis=1)\n",
    "    Dmax = np.max(Dvec)\n",
    "    ccc = c*1/Dmax\n",
    "    D = np.diag(Dvec)\n",
    "    L = D - adj\n",
    "    Ap = np.eye(sz) - ccc*L\n",
    "    A = rho * Ap\n",
    "    return A\n",
    "\n",
    "def tsg(A,tsize,x0,qsi):\n",
    "    '''\n",
    "    Generates the syntetic time series data given the connectivity matrix and the initial condiction x(0), \n",
    "    according to the dynnamical rule y(n + 1) = Ay(n) + x(n + 1)\n",
    "\n",
    "        Parameters:\n",
    "                A (2darray): Connectivity matrix\n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                x (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    sz = len(A)\n",
    "    x = np.zeros((tsize,sz))\n",
    "    \n",
    "    x[0,:] = np.ones((1,sz))*x0\n",
    "    for i in range(1,tsize):\n",
    "      for j in range(sz):\n",
    "        x[i,j] = np.dot(A[j,:],x[i-1,:])+ qsi*np.random.randn(1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def granger(z):\n",
    "    '''\n",
    "    Granger Estimator - R1*((R0)^1)\n",
    "\n",
    "        Parameters:\n",
    "                z (2darray): Time series of the observed nodes\n",
    "\n",
    "        Returns:\n",
    "                R1*inv(R0) (2darray): Estimated connectivity matrix\n",
    "    '''   \n",
    "    tsize = z.shape[1]\n",
    "    \n",
    "    #0-lag correlation matrix\n",
    "    R0=np.matmul(z,z.T)/tsize;\n",
    "\n",
    "    #1-lag correlation matrix\n",
    "    z1=z[:,1:tsize];\n",
    "    z2=z[:,0:tsize-1];\n",
    "    R1=np.matmul(z1,z2.T)/(tsize-1)\n",
    "\n",
    "    #R1*inv(R0)\n",
    "    return np.matmul(R1,np.linalg.inv(R0))\n",
    "\n",
    "def r1_minus_r3(z):\n",
    "    '''\n",
    "    NIB Estimator - R1-R3\n",
    "\n",
    "        Parameters:\n",
    "                z (2darray): Time series of the observed nodes\n",
    "\n",
    "        Returns:\n",
    "                R1-R3 (2darray): Estimated connectivity matrix\n",
    "    '''   \n",
    "    tsize = z.shape[1]\n",
    "    \n",
    "    #1-lag correlation matrix\n",
    "    z1=z[:,1:tsize];\n",
    "    z2=z[:,0:tsize-1];\n",
    "    R1=np.matmul(z1,z2.T)/(tsize-1)\n",
    "\n",
    "    #3-lag correlation matrix\n",
    "    z1=z[:,3:tsize];\n",
    "    z2=z[:,0:tsize-3];\n",
    "    R3=np.matmul(z1,z2.T)/(tsize-3);\n",
    "\n",
    "    #(R1-R3)\n",
    "    return (R1-R3)\n",
    "\n",
    "def r1(z):\n",
    "    '''\n",
    "    R1 Estimator\n",
    "\n",
    "        Parameters:\n",
    "                z (2darray): Time series of the observed nodes\n",
    "\n",
    "        Returns:\n",
    "                R1 (2darray): Estimated connectivity matrix\n",
    "    '''\n",
    "    sz = z.shape[0]\n",
    "    tsize = z.shape[1]\n",
    "    \n",
    "    #1-lag correlation matrix\n",
    "    z1=z[:,1:tsize];\n",
    "    z2=z[:,0:tsize-1];\n",
    "    R1=np.matmul(z1,z2.T)/(tsize-1)\n",
    "\n",
    "    #id - inv(R1+R2+id) Estimator\n",
    "    return R1\n",
    "\n",
    "\n",
    "def mve_perf(As,pred,estimator,undirected):\n",
    "    '''\n",
    "    Computes the estimators matrix-valued estimators performance metrics from their estimated matrix A\n",
    "\n",
    "        Parameters:\n",
    "                As (2darray): Ground truth matrix A\n",
    "                pred (2darray): Estimated matrix A\n",
    "                method (string): Clustering method to be applied\n",
    "                estimator (string): Estimator type: granger, r1-r3 or r1\n",
    "\n",
    "        Returns:\n",
    "                R1 (1darray): Performance metrics (accuracy, identifiability gap)\n",
    "    '''\n",
    "    \n",
    "    #Number of nodes\n",
    "    sz = len(As)\n",
    "\n",
    "    #Count the number of connected and disconnected pairs\n",
    "    if(undirected):\n",
    "        alln = int(sz*(sz-1)/2)\n",
    "        tr = np.triu(As)\n",
    "        np.fill_diagonal(tr,0)\n",
    "        tr = tr>0\n",
    "        tr=tr.astype(int)\n",
    "        nc = np.sum(tr)\n",
    "    else:\n",
    "        alln = (sz*sz)-sz\n",
    "        tr = As\n",
    "        np.fill_diagonal(tr,0)\n",
    "        tr = tr>0\n",
    "        tr=tr.astype(int)\n",
    "        nc = np.sum(tr)\n",
    "        \n",
    "    #Divide the pairs into connected and disconnected\n",
    "    con = np.zeros((nc))\n",
    "    dis = np.zeros((alln-nc))\n",
    "    comp = np.zeros((2,alln))\n",
    "\n",
    "    #Save data\n",
    "    c1,c2,c3 = 0,0,0\n",
    "    if(undirected):\n",
    "        for i in range(sz):\n",
    "            for j in range(i+1,sz):\n",
    "                comp[0,c1] = As[i,j]\n",
    "                comp[1,c1] = pred[i,j]\n",
    "                c1=c1+1\n",
    "                if(As[i,j]>0):\n",
    "                    con[c2] = pred[i,j]\n",
    "                    c2+=1\n",
    "                else:\n",
    "                    dis[c3] = pred[i,j]\n",
    "                    c3+=1\n",
    "    else:\n",
    "        for i in range(sz):\n",
    "            for j in range(sz):\n",
    "                if(j!=i):\n",
    "                    comp[0,c1] = As[i,j]\n",
    "                    comp[1,c1] = pred[i,j]\n",
    "                    c1=c1+1\n",
    "                    if(As[i,j]>0):\n",
    "                        con[c2] = pred[i,j]\n",
    "                        c2+=1\n",
    "                    else:\n",
    "                        dis[c3] = pred[i,j]\n",
    "                        c3+=1\n",
    "                \n",
    "    #Get the estimation performance metrics\n",
    "    accuracy=cluster_pred(np.copy(comp[0,:]),np.copy(comp[1,:]),estimator)\n",
    "    clvar,idgap=get_metrics(np.copy(comp[0,:]), np.copy(comp[1,:]), alln, nc)\n",
    "    return np.array([accuracy,clvar,idgap])\n",
    "\n",
    "\n",
    "def cluster_pred(true,pred,estimator):\n",
    "    '''\n",
    "    Clusters the estimators results into two groups and classifies it into connected and disconnected\n",
    "\n",
    "        Parameters:\n",
    "                true (2darray): Ground truth matrix A\n",
    "                pred (2darray): Estimated matrix A\n",
    "                estimator (string): Estimator type: granger, r1-r3 or r1\n",
    "\n",
    "        Returns:\n",
    "                accuracy (1darray): Accuracy of the estimations\n",
    "    '''\n",
    "\n",
    "    #Sort it\n",
    "    idx = np.argsort(pred, axis=0)\n",
    "\n",
    "    #Build a data structure for clustering\n",
    "    features = np.zeros((len(pred),2))\n",
    "    features[:,0] = np.linspace(0,1,len(pred))\n",
    "    features[:,1] = pred[idx]\n",
    "    \n",
    "    #Clustering data with Gaussian Mixture Model or kmeans\n",
    "    gmm = GaussianMixture(n_components=2)\n",
    "    labels = gmm.fit_predict(features)\n",
    "\n",
    "    #Unsort the values after clustering\n",
    "    us = np.argsort(idx, axis=0)\n",
    "    labels_unsorted = labels[us]\n",
    "    cld = pred[np.where(labels_unsorted == 1)]\n",
    "    clc = pred[np.where(labels_unsorted == 0)]\n",
    "\n",
    "    #Disconnected pairs must be on cld and conneceed pairs on clc\n",
    "    if(np.mean(cld)>np.mean(clc)):\n",
    "        temp = cld\n",
    "        cld=clc\n",
    "        clc=temp\n",
    "        labels_unsorted[:] = np.abs(labels_unsorted[:]-1)\n",
    "        \n",
    "    \n",
    "    #Count the number of pairs correctly classified\n",
    "    tcout = 0\n",
    "    for i in range(len(labels_unsorted)):\n",
    "        if((labels_unsorted[i]==1 and true[i]>0) or (labels_unsorted[i]==0 and true[i]==0)):\n",
    "            tcout+=1\n",
    "        \n",
    "    #Compute accuracy\n",
    "    accuracy = 100-(tcout/len(labels_unsorted)*100)\n",
    "    return accuracy\n",
    "\n",
    "def compare_estimators(As,z,undirected):\n",
    "    '''\n",
    "    This function computes the estimations (using the estimators) and their performance\n",
    "\n",
    "        Parameters:\n",
    "                As (2darray): Ground truth matrix A\n",
    "                z (2darray): Matrix containing the time series of the observed nodes\n",
    "                sigm (string): Clustering method to be applied\n",
    "                estimator (string): Estimator type: granger, r1-r3 or r1\n",
    "\n",
    "        Returns:\n",
    "                data (1darray): Matrix containing the performance for the different 3 estimators\n",
    "    '''\n",
    "    data = np.zeros((3,3)) \n",
    "    \n",
    "    #Granger\n",
    "    pred = granger(z)\n",
    "    data[0,:] = mve_perf(As,pred,'G',undirected)\n",
    "    \n",
    "    #R1 minus R3\n",
    "    pred = r1_minus_r3(z)\n",
    "    data[1,:] = mve_perf(As,pred,'R13',undirected)\n",
    "\n",
    "    #R1 Estimator\n",
    "    pred = r1(z)\n",
    "    data[2,:] = mve_perf(As,pred,'R12',undirected)\n",
    "    return data\n",
    "\n",
    "def get_metrics(true, pred, upper, nc):\n",
    "    '''\n",
    "    This function rescales the data and computes the performance metrics, this way the measurements are calculated using the \n",
    "    same scale\n",
    "\n",
    "        Parameters:\n",
    "                true (1darray): Ground truth matrix A\n",
    "                pred (1darray): Estimated matrix A\n",
    "                upper (int): Number of pairs of nodes\n",
    "                nc (int): Number of connected pairs\n",
    "\n",
    "        Returns:\n",
    "                idgap (int): Identifiability gap\n",
    "                clvar (int): Cluster Variance\n",
    "    '''\n",
    "    #Normalize the values\n",
    "    pred = sklearn.preprocessing.minmax_scale(pred, feature_range=(0, 1), axis=0, copy=True)\n",
    "    \n",
    "    #Initialize the structures to save the data\n",
    "    con = np.zeros((nc))\n",
    "    dis = np.zeros((upper-nc))\n",
    "    \n",
    "    #Split the values belonging to connected and disconnected pairs\n",
    "    c2,c3 = 0,0\n",
    "    for i in range(len(true)):\n",
    "        if(true[i]>0):\n",
    "            con[c2] = pred[i]\n",
    "            c2+=1\n",
    "        else:\n",
    "            dis[c3] = pred[i]\n",
    "            c3+=1\n",
    "    \n",
    "    #Threshold\n",
    "    mint = np.max(dis)\n",
    "    maxt = np.min(con)\n",
    "    threshold = np.ones((upper,))*((mint+maxt)/2)\n",
    "    \n",
    "    #Compute metrics\n",
    "    clvar = (np.var(dis)+np.var(con))/2\n",
    "    idgap = maxt - ((mint+maxt)/2)\n",
    "    \n",
    "    #If the id gap is negative then we cant correctly classify the pairs as disconnected or connected\n",
    "    if(idgap < 0):\n",
    "        idgap=0\n",
    "    \n",
    "    return clvar,idgap\n",
    "\n",
    "\n",
    "def extract_cross_correlation(zz,n):\n",
    "    '''\n",
    "    Extracts the cross correlation from the time series\n",
    "\n",
    "        Parameters:\n",
    "                zz (1darray): Matrix with the observed node time series\n",
    "                n (int): Number of nodes in the graph\n",
    "        Returns:\n",
    "                data_nn (2darray): Matrix containing a fecture vector for each pair of nodes\n",
    "                y (1darray): Ground-truth\n",
    "    '''\n",
    "    tsize=zz.shape[0]\n",
    "    upper = int(n*(n-1)/2)\n",
    "    data_nn = np.zeros((200,upper))\n",
    "    target = np.zeros((1,upper))\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    #Go through each pair and compute the time laged cross-correlation\n",
    "    for j in range(n):\n",
    "        for k in range(j+1,n):\n",
    "            aux = signal.correlate(zz[:,j],zz[:,k], mode=\"full\")\n",
    "            data_nn[:,counter] = aux[tsize-100:tsize+100]\n",
    "            target[0,counter] = A[j,k]\n",
    "            counter = counter + 1\n",
    "\n",
    "    data_nn = data_nn/np.max(data_nn)\n",
    "\n",
    "    y=target>0\n",
    "    y=y.astype(int)+1\n",
    "\n",
    "    return data_nn.T,y.T\n",
    "  \n",
    "def npairs():\n",
    "    '''\n",
    "    Counts the number of connected and disconnected pairs in the observable nodes\n",
    "    '''\n",
    "    upper = int((se-ss)*((se-ss)-1)/2)\n",
    "    tr = np.triu(As)\n",
    "    np.fill_diagonal(tr,0)\n",
    "    tr = tr>0\n",
    "    tr=tr.astype(int)\n",
    "    nc = np.sum(tr)\n",
    "    return upper,nc\n",
    "    \n",
    "def plot_svm_results(data_nn,y):\n",
    "    '''\n",
    "    Loads a saved SVM model and classfies the features into connected or disconnected classes\n",
    "\n",
    "        Parameters:\n",
    "                model_name (string): Name of the saved model\n",
    "                data_nn (1darray): Features of the observed models\n",
    "                y (1darray): Ground-truth\n",
    "        Returns:\n",
    "                [per,clvar,idgap] (1darray): Performance of the SVM predictions\n",
    "    '''\n",
    "    pred = svm_model.predict(data_nn)\n",
    "    true = y.flatten()\n",
    "    \n",
    "    #Compute the metrics\n",
    "    upper,nc = npairs()\n",
    "    clvar,idgap=get_metrics(np.copy(true-1), np.copy(pred), upper, nc)\n",
    "\n",
    "    per=100-(sum(pred!=true)/len(pred)*100)\n",
    "    return [per,clvar,idgap]\n",
    "\n",
    "def load_model(name):\n",
    "    '''\n",
    "    Loads a saved SVM model\n",
    "\n",
    "        Parameters:\n",
    "                name (string): Name of the saved model\n",
    "        Returns:\n",
    "                model (keras.object):SVM Model\n",
    "    '''\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "def plot_nn_results(data_nn,y):\n",
    "    '''\n",
    "    Loads a saved CNN model and classfies the features into connected or disconnected classes\n",
    "\n",
    "        Parameters:\n",
    "                model_name (string): Name of the saved model\n",
    "                data_nn (1darray): Features of the observed models\n",
    "                y (1darray): Ground-truth\n",
    "        Returns:\n",
    "                [accuracy_nn,clvar,idgap] (1darray): Performance of the CNN predictions\n",
    "    '''\n",
    "    \n",
    "    #Divide the connected and disconnected pairs values\n",
    "    idd = y < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    idc = y > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Reshape the data to 3 dimensions\n",
    "    data_nn = data_nn.reshape((data_nn.shape[0], data_nn.shape[1], 1))\n",
    "\n",
    "    #Predicts the weight of the connection and classifies the data\n",
    "    dis_mpred = cnn_model.predict(data_nn[idd,:,:],verbose=0)\n",
    "    con_mpred = cnn_model.predict(data_nn[idc,:,:],verbose=0)\n",
    "\n",
    "    truec_nn = np.sum(dis_mpred<1.5)\n",
    "    trued_nn = np.sum(con_mpred>1.5)\n",
    "    tall = truec_nn + trued_nn\n",
    "\n",
    "    accuracy_nn = tall/(len(dis_mpred)+len(con_mpred))*100\n",
    "\n",
    "    #Compute the metrics\n",
    "    \n",
    "    upper,nc = npairs()\n",
    "    clvar,idgap=get_metrics(np.copy(y-1), np.copy(cnn_model.predict(data_nn,verbose=0)), upper, nc)\n",
    "    return [accuracy_nn,clvar,idgap]\n",
    "    \n",
    "    \n",
    "def save_pickles(name,comparison_data,sz_main,p):\n",
    "    '''\n",
    "    Saves the performance metrics of all estimators\n",
    "\n",
    "        Parameters:\n",
    "                comparison_data (2darray): Performance of different estimators over a range of number of samples\n",
    "                sz_main (int): Number of observed nodes\n",
    "                p (1darray): Probability of the Erdős–Rényi Graph Model\n",
    "    '''\n",
    "    output = open(name, 'wb')\n",
    "    pickle.dump(comparison_data, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load real-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_edges(path,nnodes):\n",
    "    adj = np.zeros((nnodes,nnodes))\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    for i in range(len(lines)):\n",
    "        splt = lines[i].split()\n",
    "        edge1= int(splt[0])-1\n",
    "        edge2= int(splt[1])-1\n",
    "        adj[edge1,edge2] = 1\n",
    "    return adj\n",
    "\n",
    "def load_network(name):\n",
    "    if(name == \"enzyme.edges\"):\n",
    "        adj = load_from_edges(\"../data/\"+name,122)\n",
    "    elif(name == \"macaque.edges\"):\n",
    "        adj = load_from_edges(\"../data/\"+name,93)\n",
    "    return adj\n",
    "    \n",
    "enzyme_net = \"enzyme.edges\"\n",
    "macaque_net = \"macaque.edges\"\n",
    "\n",
    "adj = load_network(enzyme_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads the models\n",
    "cnn_model = load_model(\"../models/cnn\")\n",
    "svm_model =     loaded_model = pickle.load(open(\"../models/svm.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paratemers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "undirected = True\n",
    "    \n",
    "#Define the range of noise variance\n",
    "qsi = 0.1\n",
    "x0 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation parameters\n",
    "nruns=4 #Number of runs\n",
    "min_samples = 1000\n",
    "tsize = 4000\n",
    "max_samples=tsize\n",
    "step=1\n",
    "tssize = np.arange(round(min_samples/1000),round(max_samples/1000),step=step)  #Number of samples (time series size) for each test\n",
    "nestimators = 6 #Number of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sz_main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SRGIOM~1\\AppData\\Local\\Temp/ipykernel_2516/3522585064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Run:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0msave_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'real_world_perf.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0msave_pickles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcomparison_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msz_main\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sz_main' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Makes several runs for given graph over a range of number of samples\n",
    "\n",
    "-At each run a new adjancency and A matrix is generated for the Erdős–Rényi Graph Model previously defined and then the time\n",
    "series are generated according to the dynamics defined\n",
    "-To observe the performance of the estimators in function of the number of samples, the performance if measure over a range of \n",
    "number of samples\n",
    "'''\n",
    "\n",
    "#Allocate memory to save the performance data\n",
    "comparison_data = np.zeros((len(tssize),nruns,nestimators*3))\n",
    "\n",
    "#Make several runs\n",
    "for i in range(nruns):\n",
    "    #Generate an A matrix\n",
    "    A = get_A(adj,c,rho)\n",
    "\n",
    "    #Generate the time series\n",
    "    data2 = tsg(A,tsize,x0,qsi)\n",
    "    \n",
    "    for j in range(len(tssize)):\n",
    "        data_aux=np.copy(data2)\n",
    "        data=data_aux[0:(tssize[j]*1000),:]\n",
    "\n",
    "        #Select the observable nodes\n",
    "        ss = 0\n",
    "        se = 20\n",
    "        #Select the observable nodes nada\n",
    "        z=data[:,ss:se]\n",
    "        As = A[ss:se,ss:se]\n",
    "\n",
    "        #Compare algebrical estimaros\n",
    "        estimators_acc = compare_estimators(As,z.T,undirected)\n",
    "        comparison_data[j,i,0:3] = np.array(estimators_acc[:,0])\n",
    "        comparison_data[j,i,6:9] = np.array(estimators_acc[:,1])\n",
    "        comparison_data[j,i,12:15] = np.array(estimators_acc[:,2])\n",
    "\n",
    "        #Extract the cross-correlation\n",
    "        data_nn,y = extract_cross_correlation(z,se-ss)\n",
    "\n",
    "        #Plot svm results\n",
    "        stats_svm=plot_svm_results(data_nn,y)\n",
    "        comparison_data[j,i,3] = np.array(stats_svm[0])\n",
    "        comparison_data[j,i,9] = np.array(stats_svm[1])\n",
    "        comparison_data[j,i,15] = np.array(stats_svm[2])\n",
    "\n",
    "        #Plot cnn results\n",
    "        stats_cnn = plot_nn_results(data_nn,y)\n",
    "        comparison_data[j,i,4] = np.array(stats_cnn[0])\n",
    "        comparison_data[j,i,10] = np.array(stats_cnn[1])\n",
    "        comparison_data[j,i,16] = np.array(stats_cnn[2])\n",
    "    \n",
    "    print(\"Run:\", i)\n",
    "    save_name='real_world_perf.pkl'\n",
    "    save_pickles(save_name,comparison_data,sz_main,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    save_name='real_world_perf.pkl'\n",
    "    save_pickles(save_name,comparison_data,sz_main,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickles(name):\n",
    "    pkl_file = open(name, 'rb')\n",
    "    data = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return data\n",
    "\n",
    "save_name='real_world_perf.pkl'\n",
    "data=load_pickles(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x-axis length\n",
    "x = np.arange(min_samples,tsize,step=step*1000)\n",
    "ni=0\n",
    "nx=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_plot = np.zeros((data.shape[0],5))\n",
    "for i in range(data.shape[0]):\n",
    "    data_plot[i,:] = np.mean(data[i,:,0:5],axis=0)\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.grid(alpha=0.5,linestyle='--')\n",
    "plt.plot(x[ni:nx],data_plot[ni:nx,4],color='k',linestyle='-',linewidth=2)\n",
    "plt.plot(x[ni:nx],data_plot[ni:nx,3],color='tab:red',linestyle='-.',linewidth=2)\n",
    "plt.plot(x[ni:nx],data_plot[ni:nx,0],color='tab:blue',linestyle='--',linewidth=2)\n",
    "plt.plot(x[ni:nx],data_plot[ni:nx,2],color='tab:green',linestyle=':', dashes=(1,1),linewidth=2.5)\n",
    "plt.plot(x[ni:nx],data_plot[ni:nx,1],color='tab:orange',linestyle=':',dashes=(1,3),linewidth=2.5)\n",
    "plt.ticklabel_format(axis=\"x\", style=\"sci\", scilimits=(0,0))\n",
    "plt.title(r'$N=50, p=0.2$')\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Number of samples (n)\")\n",
    "plt.legend([\"NonLinear CNN\", \"Linear SVM\",\"Granger\",\"R1\",\"R1-R3\"],loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_Noise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
