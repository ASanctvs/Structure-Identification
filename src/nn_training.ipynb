{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required packages to run the code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "sq1KN4nrSOu_",
    "outputId": "3e30ae80-61ec-4608-c95f-4cd2d7437c45"
   },
   "outputs": [],
   "source": [
    "def get_adjacency(sz,p,undirected):\n",
    "    '''\n",
    "    Generates a realization of an Erdős–Rényi Graph Model, undirected or directed.\n",
    "    -First generates of matrix of random floating point numbers in the range [0.0, 1.0].\n",
    "    -If those values are <=p then there is no edge between pairs\n",
    "    -Makes the matrix symmetric if the graoh is undirected\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "\n",
    "        Returns:\n",
    "                adj (2darray): Adjacency matrix\n",
    "    '''\n",
    "    adj = np.random.random((sz, sz)) <= p\n",
    "    adj = np.triu(adj.astype(int))\n",
    "    np.fill_diagonal(adj,0)\n",
    "    if(undirected):\n",
    "        adj = adj + adj.T\n",
    "    return adj\n",
    "\n",
    "def get_A(adj,c,rho):\n",
    "    '''\n",
    "    Generates the connectivity matrix (interaction weights) from the adjacency matrix according to the laplacian rule\n",
    "\n",
    "        Parameters:\n",
    "                adj (2darray): Adjacency matrix\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1\n",
    "\n",
    "        Returns:\n",
    "                A (2darray): Connectivity matrix\n",
    "    '''    \n",
    "    sz = len(adj)\n",
    "    Dvec = np.sum(adj, axis=1)\n",
    "    Dmax = np.max(Dvec)\n",
    "    ccc = c*1/Dmax\n",
    "    D = np.diag(Dvec)\n",
    "    L = D - adj\n",
    "    Ap = np.eye(sz) - ccc*L\n",
    "    A = rho * Ap\n",
    "    return A\n",
    "\n",
    "def tsg(A,tsize,x0,qsi):\n",
    "    '''\n",
    "    Generates the syntetic time series data given the connectivity matrix and the initial condiction x(0), \n",
    "    according to the dynnamical rule y(n + 1) = Ay(n) + x(n + 1)\n",
    "\n",
    "        Parameters:\n",
    "                A (2darray): Connectivity matrix\n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                x (2darray): Time series data of the graph\n",
    "    ''' \n",
    "    sz = len(A)\n",
    "    x = np.zeros((tsize,sz))\n",
    "    \n",
    "    x[0,:] = np.ones((1,sz))*x0\n",
    "    for i in range(1,tsize):\n",
    "      for j in range(sz):\n",
    "        x[i,j] = np.dot(A[j,:],x[i-1,:]) + qsi*np.random.randn(1)\n",
    "    return x\n",
    "\n",
    "def create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected):\n",
    "    '''\n",
    "    Generates the synthectic data, extracts the features and returns the tranning/testing dataset\n",
    "\n",
    "        Parameters:\n",
    "                sz (int): Number of nodes\n",
    "                p (int): Probability of existing an edge between each pair of nodes\n",
    "                c,rho (int): Numbers between 0 and 1, to make the spectral radius < 1  \n",
    "                tsize (int): Time series size - number of samples\n",
    "                x0 (int): Initial condition x(0), in this case is zero\n",
    "                qsi (int): Noise standart deviation \n",
    "\n",
    "        Returns:\n",
    "                data (2darray): Matrix containing the feature-vectors between each pair of nodes\n",
    "                target (1darray): Ground-truth - pairs are connected or disconnected\n",
    "    '''     \n",
    "    #Generate the adjacency and A matrices\n",
    "    adj = get_adjacency(sz,p,undirected)\n",
    "    A = get_A(adj,c,rho)\n",
    "   \n",
    "    #Is the graph undirected or directed\n",
    "    if(undirected):\n",
    "        \n",
    "        #Create data structures\n",
    "        upper = int(sz*(sz-1)/2)  #Number of elements in the upper matrix\n",
    "        data = np.zeros((200,upper))\n",
    "        target = np.zeros((1,upper))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair (of the upper matrix) and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(j+1,sz):\n",
    "                #Compute the cross correlation\n",
    "                aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                #Extracts the first negative and positive lags\n",
    "                data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                #Saves the data\n",
    "                target[0,counter] = A[j,k]\n",
    "                counter = counter + 1\n",
    "    else:\n",
    "        #Create data structures\n",
    "        dsize = (sz*sz)-sz       #Number of elements excluding the diagonal\n",
    "        data = np.zeros((200,dsize))\n",
    "        target = np.zeros((1,dsize))\n",
    "\n",
    "        #Generates the synthetic time series\n",
    "        x = tsg(A,tsize,x0,qsi)\n",
    "        \n",
    "        #Goes through each pair and computes the time laged cross-correlation (excludes diagonal)\n",
    "        counter = 0\n",
    "        for j in range(sz):\n",
    "            for k in range(sz):\n",
    "                if(j!=k):\n",
    "                    #Computes the cross correlation\n",
    "                    aux = signal.correlate(x[:,j],x[:,k], mode=\"full\")\n",
    "                    #Extracts the firs negative and positive lags\n",
    "                    data[:,counter] = aux[tsize-100:tsize+100]\n",
    "                    #Saves the data\n",
    "                    target[0,counter] = A[j,k]\n",
    "                    counter = counter + 1 \n",
    "    return data,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "sz = 50     #Number of nodes\n",
    "p = 0.5      #Probability of nodes being connected (Erdős–Rényi)\n",
    "c = 0.9\n",
    "rho = 0.75\n",
    "\n",
    "#Define the range of noise variance\n",
    "qsi = 0.1\n",
    "tsize = 100000    #Number of samples (time series size)\n",
    "x0 = 0            #Initial condition\n",
    "   \n",
    "#True if the graph is undirected, False if not\n",
    "undirected = True\n",
    "    \n",
    "#Generate a dataset\n",
    "#Data has a n x m shape, where n is the number of features and m the number of samples\n",
    "data,target = create_dataset(sz,p,c,rho,tsize,x0,qsi,undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "data_scaled = data/np.max(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a single model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 39ms/step - loss: 0.4752 - val_loss: 0.1279\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0668 - val_loss: 0.0138\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0282 - val_loss: 0.0111\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0223 - val_loss: 0.0128\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0201 - val_loss: 0.0091\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0189 - val_loss: 0.0082\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0164 - val_loss: 0.0076\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0136 - val_loss: 0.0071\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0178 - val_loss: 0.0065\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0121 - val_loss: 0.0071\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0112 - val_loss: 0.0072\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0068 - val_loss: 0.0029\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0064 - val_loss: 0.0022\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0055 - val_loss: 0.0030\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0032 - val_loss: 0.0013\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0021 - val_loss: 8.1032e-04\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0011 - val_loss: 5.7269e-04\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 8.7829e-04 - val_loss: 4.2758e-04\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.0964e-04 - val_loss: 5.6580e-04\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 5.3887e-04 - val_loss: 2.4960e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 4.6422e-04 - val_loss: 1.7969e-04\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 3.7309e-04 - val_loss: 1.9934e-04\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 3.3593e-04 - val_loss: 3.5757e-04\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 3.2220e-04 - val_loss: 2.0669e-04\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 3.5950e-04 - val_loss: 3.3296e-04\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.6475e-04 - val_loss: 4.1509e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 3.2665e-04 - val_loss: 4.2240e-04\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 3.1557e-04 - val_loss: 1.8523e-04\n"
     ]
    }
   ],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model\n",
    " \n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "#Split the data for trainning and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "#Reshapes the data into three dimensions\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "#Train the model\n",
    "model = train_model(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "Accuracy:  100.0 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the testing results\n",
    "#Disconnected samples\n",
    "idd = y_test < 2\n",
    "idd = np.squeeze(idd)\n",
    "\n",
    "#Connected samples\n",
    "idc = y_test > 1\n",
    "idc = np.squeeze(idc)\n",
    "\n",
    "#Threshold in the middle\n",
    "threshold = np.ones((X_test.shape[0]))*1.5\n",
    "\n",
    "#Predict the samples seperaly\n",
    "dpred = model.predict(X_test[idd,:,:])\n",
    "cpred =model.predict(X_test[idc,:,:])\n",
    "\n",
    "fig=plt.figure()\n",
    "plt.plot(np.arange(0,len(dpred)),dpred,'.r')\n",
    "plt.plot(np.arange(len(dpred),len(dpred)+len(cpred)),cpred,'.g')\n",
    "plt.plot(threshold,'--b')\n",
    "plt.legend([\"Disconnected\",\"Connected\"],loc='best')\n",
    "plt.ylabel(\"Estimation\")\n",
    "\n",
    "trued = np.sum(dpred<1.5)\n",
    "truec = np.sum(cpred>1.5)\n",
    "true = trued + truec\n",
    "accuracy_nn = true/(len(dpred)+len(cpred))*100\n",
    "print(\"Accuracy:  \" + str(accuracy_nn) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN model\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='model'\n",
    "if(save):\n",
    "    save_model(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Train and test a SVM with linear kernel\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(data_scaled[:,:].T, y.flatten())\n",
    "pred = clf.predict(data_scaled.T)\n",
    "true = y.flatten()\n",
    "print(\"Accuracy:  \" + str(sum(pred==true)/len(pred)*100) + \" %\")\n",
    "\n",
    "#Save the model?\n",
    "save=False\n",
    "if(save):\n",
    "    filename = 'svm_model.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multiple models select the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train,X_test,y_train,y_test):\n",
    "    cb = EarlyStopping(monitor='val_loss', mode='min',patience=7)\n",
    "\n",
    "    #CNN architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, strides=2,activation='relu', input_shape=(200,1)))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(200, activation='tanh'))\n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    #Save the rmsesparse_categorical_crossentropy\n",
    "    history = model.fit(X_train, y_train, epochs=1000, validation_split=0.2, verbose=1,callbacks=[cb])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 37ms/step - loss: 0.4235 - val_loss: 0.0372\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0481 - val_loss: 0.0112\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0355 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0289 - val_loss: 0.0081\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0229 - val_loss: 0.0076\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0212 - val_loss: 0.0064\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0154 - val_loss: 0.0065\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0146 - val_loss: 0.0083\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0139 - val_loss: 0.0040\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0130 - val_loss: 0.0049\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0109 - val_loss: 0.0030\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0092 - val_loss: 0.0023\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0069 - val_loss: 0.0022\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0066 - val_loss: 0.0048\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0057 - val_loss: 0.0025\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0062 - val_loss: 0.0050\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0059 - val_loss: 0.0024\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0064 - val_loss: 0.0040\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0048 - val_loss: 0.0017\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0059 - val_loss: 8.9145e-04\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0045 - val_loss: 0.0033\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0052 - val_loss: 4.3887e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0059 - val_loss: 5.7423e-04\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0044 - val_loss: 0.0015\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0036 - val_loss: 3.4983e-04\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0036 - val_loss: 3.6096e-04\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0040 - val_loss: 7.8332e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0046 - val_loss: 4.3142e-04\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0029 - val_loss: 2.9038e-04\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0033 - val_loss: 5.8466e-04\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0027 - val_loss: 3.8896e-04\n",
      "Epoch 33/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0030 - val_loss: 1.7338e-04\n",
      "Epoch 34/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0031 - val_loss: 0.0048\n",
      "Epoch 35/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0029 - val_loss: 3.7375e-04\n",
      "Epoch 36/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0019 - val_loss: 4.2823e-04\n",
      "Epoch 37/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0023 - val_loss: 8.5632e-04\n",
      "Epoch 38/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0015 - val_loss: 4.2352e-04\n",
      "Epoch 39/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.4175e-04 - val_loss: 9.7727e-04\n",
      "Epoch 40/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0012 - val_loss: 7.9530e-04\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 37ms/step - loss: 0.5073 - val_loss: 0.1083\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0617 - val_loss: 0.0261\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0323 - val_loss: 0.0124\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0244 - val_loss: 0.0179\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0291 - val_loss: 0.0078\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0190 - val_loss: 0.0131\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0157 - val_loss: 0.0082\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0159 - val_loss: 0.0063\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0140 - val_loss: 0.0052\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0148 - val_loss: 0.0059\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.0119 - val_loss: 0.0048\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0117 - val_loss: 0.0039\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0107 - val_loss: 0.0033\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0077 - val_loss: 0.0029\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0078 - val_loss: 0.0026\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.0080 - val_loss: 0.0127\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.0096 - val_loss: 0.0108\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 0.0073 - val_loss: 0.0019\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0047 - val_loss: 0.0014\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0040 - val_loss: 0.0015\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0034 - val_loss: 8.3245e-04\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0029 - val_loss: 7.3182e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 0.0030 - val_loss: 7.0849e-04\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0031 - val_loss: 0.0062\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0023 - val_loss: 3.5301e-04\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0014 - val_loss: 2.1773e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.5640e-04 - val_loss: 3.1620e-04\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 6.0502e-04 - val_loss: 1.1909e-04\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 3.2683e-04 - val_loss: 3.3846e-05\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 4.9390e-04 - val_loss: 6.1837e-05\n",
      "Epoch 33/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.6485e-04 - val_loss: 1.3511e-04\n",
      "Epoch 34/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.8539e-04 - val_loss: 2.3050e-05\n",
      "Epoch 35/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.8152e-04 - val_loss: 1.9106e-04\n",
      "Epoch 36/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.0022e-04 - val_loss: 4.8523e-05\n",
      "Epoch 37/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.8153e-04 - val_loss: 6.5580e-04\n",
      "Epoch 38/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.3312e-04 - val_loss: 2.4009e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.6764e-04 - val_loss: 2.2279e-05\n",
      "Epoch 40/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.7747e-04 - val_loss: 1.9469e-05\n",
      "Epoch 41/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.4315e-04 - val_loss: 2.3606e-05\n",
      "Epoch 42/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.5866e-04 - val_loss: 2.0959e-05\n",
      "Epoch 43/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.0815e-04 - val_loss: 1.0940e-04\n",
      "Epoch 44/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.1664e-04 - val_loss: 2.3476e-05\n",
      "Epoch 45/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.1364e-04 - val_loss: 2.6162e-04\n",
      "Epoch 46/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.9213e-04 - val_loss: 6.1063e-05\n",
      "Epoch 47/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.4944e-04 - val_loss: 4.9285e-05\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 44ms/step - loss: 0.4592 - val_loss: 0.1120\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0637 - val_loss: 0.0336\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0321 - val_loss: 0.0095\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0233 - val_loss: 0.0086\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0238 - val_loss: 0.0154\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0291 - val_loss: 0.0067\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0169 - val_loss: 0.0063\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0164 - val_loss: 0.0058\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0145 - val_loss: 0.0089\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0137 - val_loss: 0.0051\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0106 - val_loss: 0.0041\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0094 - val_loss: 0.0035\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0107 - val_loss: 0.0036\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0080 - val_loss: 0.0056\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0083 - val_loss: 0.0022\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0065 - val_loss: 0.0018\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0053 - val_loss: 0.0017\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0056 - val_loss: 0.0017\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0049 - val_loss: 0.0022\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0042 - val_loss: 6.0609e-04\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0027 - val_loss: 7.0022e-04\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0026 - val_loss: 3.9531e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0025 - val_loss: 4.9405e-04\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0017 - val_loss: 9.6430e-04\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0010 - val_loss: 2.8494e-04\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 6.4165e-04 - val_loss: 1.0735e-04\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 4.9872e-04 - val_loss: 1.5621e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 3.6919e-04 - val_loss: 4.0459e-04\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.9239e-04 - val_loss: 1.1104e-04\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.2779e-04 - val_loss: 3.7784e-04\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 5.0340e-04 - val_loss: 1.3267e-04\n",
      "Epoch 33/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.9580e-04 - val_loss: 3.7213e-04\n",
      "Epoch 34/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.4069e-04 - val_loss: 4.1097e-05\n",
      "Epoch 35/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 1.9278e-04 - val_loss: 2.2797e-05\n",
      "Epoch 36/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.0326e-04 - val_loss: 3.7213e-04\n",
      "Epoch 37/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.8611e-04 - val_loss: 5.0153e-04\n",
      "Epoch 38/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.4946e-04 - val_loss: 0.0014\n",
      "Epoch 39/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.4422e-04 - val_loss: 1.2970e-04\n",
      "Epoch 40/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.5851e-04 - val_loss: 2.9258e-04\n",
      "Epoch 41/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.0839e-04 - val_loss: 1.7351e-05\n",
      "Epoch 42/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.1044e-04 - val_loss: 1.3313e-05\n",
      "Epoch 43/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.0180e-04 - val_loss: 8.0205e-06\n",
      "Epoch 44/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 7.7952e-05 - val_loss: 8.6848e-06\n",
      "Epoch 45/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.5291e-05 - val_loss: 2.0405e-05\n",
      "Epoch 46/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.8710e-05 - val_loss: 1.0132e-05\n",
      "Epoch 47/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.3826e-04 - val_loss: 1.9183e-04\n",
      "Epoch 48/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 7.5600e-05 - val_loss: 1.0422e-04\n",
      "Epoch 49/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.3111e-05 - val_loss: 9.5790e-06\n",
      "Epoch 50/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 1.3217e-04 - val_loss: 0.0012\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 38ms/step - loss: 0.5021 - val_loss: 0.0893\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0483 - val_loss: 0.0160\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0264 - val_loss: 0.0142\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0225 - val_loss: 0.0079\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0239 - val_loss: 0.0134\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0196 - val_loss: 0.0062\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0169 - val_loss: 0.0054\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0148 - val_loss: 0.0056\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0167 - val_loss: 0.0073\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0160 - val_loss: 0.0067\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0107 - val_loss: 0.0033\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0099 - val_loss: 0.0027\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0088 - val_loss: 0.0037\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0073 - val_loss: 0.0055\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0071 - val_loss: 0.0023\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0059 - val_loss: 0.0011\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0053 - val_loss: 0.0017\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0049 - val_loss: 5.4548e-04\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0051 - val_loss: 8.1853e-04\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0043 - val_loss: 0.0029\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0059 - val_loss: 0.0039\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0049 - val_loss: 9.5916e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0037 - val_loss: 7.6036e-04\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0037 - val_loss: 4.7152e-04\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0039 - val_loss: 2.8532e-04\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0043 - val_loss: 0.0011\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0038 - val_loss: 4.3899e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0042 - val_loss: 0.0024\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0038 - val_loss: 4.3318e-04\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0046 - val_loss: 0.0029\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "Epoch 1/1000\n",
      "25/25 [==============================] - 2s 38ms/step - loss: 0.5176 - val_loss: 0.0914\n",
      "Epoch 2/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0557 - val_loss: 0.0088\n",
      "Epoch 3/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0293 - val_loss: 0.0082\n",
      "Epoch 4/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0248 - val_loss: 0.0143\n",
      "Epoch 5/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0224 - val_loss: 0.0066\n",
      "Epoch 6/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0169 - val_loss: 0.0059\n",
      "Epoch 7/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 8/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0161 - val_loss: 0.0051\n",
      "Epoch 9/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0118 - val_loss: 0.0068\n",
      "Epoch 10/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0116 - val_loss: 0.0067\n",
      "Epoch 11/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0118 - val_loss: 0.0064\n",
      "Epoch 12/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0089 - val_loss: 0.0037\n",
      "Epoch 13/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0096 - val_loss: 0.0047\n",
      "Epoch 14/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0082 - val_loss: 0.0023\n",
      "Epoch 15/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0053 - val_loss: 0.0022\n",
      "Epoch 16/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0054 - val_loss: 0.0030\n",
      "Epoch 17/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0043 - val_loss: 9.4773e-04\n",
      "Epoch 18/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0026 - val_loss: 6.9003e-04\n",
      "Epoch 19/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0025 - val_loss: 0.0015\n",
      "Epoch 20/1000\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0016 - val_loss: 1.9318e-04\n",
      "Epoch 21/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0012 - val_loss: 2.3719e-04\n",
      "Epoch 22/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 5.7387e-04 - val_loss: 1.0122e-04\n",
      "Epoch 23/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 4.3765e-04 - val_loss: 8.5443e-05\n",
      "Epoch 24/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 4.3323e-04 - val_loss: 2.8866e-04\n",
      "Epoch 25/1000\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 2.7453e-04 - val_loss: 8.3735e-05\n",
      "Epoch 26/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 2.2066e-04 - val_loss: 1.1190e-04\n",
      "Epoch 27/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 2.1910e-04 - val_loss: 2.8128e-05\n",
      "Epoch 28/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.4866e-04 - val_loss: 3.8446e-04\n",
      "Epoch 29/1000\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 6.1280e-04 - val_loss: 3.5982e-05\n",
      "Epoch 30/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 6.3363e-04 - val_loss: 5.4156e-04\n",
      "Epoch 31/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.8424e-04 - val_loss: 4.3630e-05\n",
      "Epoch 32/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.4678e-04 - val_loss: 2.0843e-05\n",
      "Epoch 33/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 2.0690e-04 - val_loss: 3.0356e-04\n",
      "Epoch 34/1000\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 2.3211e-04 - val_loss: 1.2976e-04\n",
      "Epoch 35/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 1.4468e-04 - val_loss: 2.1716e-05\n",
      "Epoch 36/1000\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.0970e-04 - val_loss: 8.2384e-04\n",
      "Epoch 37/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 6.5776e-04 - val_loss: 5.6515e-04\n",
      "Epoch 38/1000\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.8393e-04 - val_loss: 7.2054e-05\n",
      "Epoch 39/1000\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 1.5341e-04 - val_loss: 2.7222e-05\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "nruns=5\n",
    "models_list = []\n",
    "performance_list = []\n",
    "\n",
    "#Transforms the ground-truth values into classes\n",
    "y=target>0\n",
    "y=y.astype(int)+1\n",
    "\n",
    "\n",
    "for i in range(nruns):\n",
    "    #Split the data for trainning and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_scaled.T,y.T, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Reshapes the data into three dimensions\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    #Train the model\n",
    "    model = train_model(X_train,X_test,y_train,y_test)\n",
    "    \n",
    "    #Save the data\n",
    "    models_list.append(model)\n",
    "    \n",
    "    #Compute the performance according to a chosen metric on the testing set\n",
    "    #I chose the identifiability in this case\n",
    "    #Disconnected samples\n",
    "    idd = y_test < 2\n",
    "    idd = np.squeeze(idd)\n",
    "\n",
    "    #Connected samples\n",
    "    idc = y_test > 1\n",
    "    idc = np.squeeze(idc)\n",
    "\n",
    "    #Predict the samples seperaly\n",
    "    dpred = model.predict(X_test[idd,:,:])\n",
    "    cpred =model.predict(X_test[idc,:,:])\n",
    "    \n",
    "    id_gap = min(cpred)-max(dpred)\n",
    "    \n",
    "    #Add to performance list\n",
    "    performance_list.append(id_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  2\n"
     ]
    }
   ],
   "source": [
    "#Select the best model\n",
    "idx = performance_list.index(max(performance_list))\n",
    "best_model = models_list[idx]\n",
    "print(\"Index:  \" + str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save or load a trainned CNN\n",
    "def save_model(model,name):\n",
    "    model.save(name)\n",
    "\n",
    "def load_model(name):\n",
    "    return keras.models.load_model(name)\n",
    "\n",
    "#Save = True to save the model\n",
    "save=False\n",
    "model_name='model'\n",
    "if(save):\n",
    "    save_model(best_model,model_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
